{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATTERN RECOGNITION COURSEWORK 2\n",
    "## REPRESENTATION AND DISTANCE METRIC LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from metric_learn.lmnn import LMNN\n",
    "from metric_learn.covariance import Covariance\n",
    "from metric_learn.mlkr import MLKR\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random see so that results are replicable\n",
    "random.seed(9999999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access raw data from files\n",
    "train_idxs = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['train_idx'].flatten()-1\n",
    "query_idxs = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['query_idx'].flatten()-1\n",
    "gallery_idxs = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['gallery_idx'].flatten()-1\n",
    "labels = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['labels'].flatten()\n",
    "camId = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['camId'].flatten()\n",
    "filelist = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')['filelist'].flatten()\n",
    "with open('PR_data/feature_data.json','r') as f:\n",
    "    features = json.load(f)\n",
    "features = np.array(features) # rows: pictures, columns: features (one row contains one image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the usable data sets from the given indices\n",
    "train_feat = features[train_idxs,:]\n",
    "train_labels = labels[train_idxs]\n",
    "train_camId = camId[train_idxs]\n",
    "query_feat = features[query_idxs,:]\n",
    "query_labels = labels[query_idxs]\n",
    "query_camId = camId[query_idxs]\n",
    "gallery_feat = features[gallery_idxs,:]\n",
    "gallery_labels = labels[gallery_idxs]\n",
    "gallery_camId = camId[gallery_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set\n",
    "unique_id_train = np.unique(train_labels) #find all unique training IDs\n",
    "unique_id_val = unique_id_train[np.array(random.sample(range(unique_id_train.shape[0]),100))] # pick 100 random unique training IDs\n",
    "bool_idx_val = np.isin(train_labels, unique_id_val)\n",
    "bool_idx_train_noval = np.isin(train_labels, unique_id_val, invert=True)\n",
    "\n",
    "train_noval_feat = train_feat[bool_idx_train_noval,:]\n",
    "train_noval_labels = train_labels[bool_idx_train_noval]\n",
    "train_noval_camId = train_camId[bool_idx_train_noval]\n",
    "train_noval_filelist_idxs = train_idxs[bool_idx_train_noval]\n",
    "\n",
    "val_feat = train_feat[bool_idx_val,:]\n",
    "val_labels = train_labels[bool_idx_val]\n",
    "val_camId = train_camId[bool_idx_val]\n",
    "val_filelist_idxs = train_idxs[bool_idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Validation Query and Gallery set: </b> <br>\n",
    "Make sure that the query and gallery for validation are similarly structured as the gallery and query set for testing (i.e. take one picture from each camera and person for the query set - only valid if that sample has at least one complementary image from the other camera in the gallery set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gallery and query set for validation\n",
    "val_label_cam_pair = np.append(val_labels.reshape((1,val_labels.shape[0])),val_camId.reshape((1,val_camId.shape[0])),axis=0)\n",
    "val_pair_unique, val_pair_unique_indxs = np.unique(val_label_cam_pair, axis=1, return_index=True)\n",
    "\n",
    "val_query_feat = val_feat[val_pair_unique_indxs,:]\n",
    "val_query_labels = val_labels[val_pair_unique_indxs]\n",
    "val_query_camId = val_camId[val_pair_unique_indxs]\n",
    "\n",
    "val_gallery_feat = np.delete(val_feat, val_pair_unique_indxs, axis = 0)\n",
    "val_gallery_labels = np.delete(val_labels, val_pair_unique_indxs)\n",
    "val_gallery_camId = np.delete(val_camId, val_pair_unique_indxs)\n",
    "\n",
    "idxs_delete = []\n",
    "for i in range(val_query_labels.shape[0]):\n",
    "    g_camId = val_gallery_camId[val_gallery_labels==val_query_labels[i]]\n",
    "    if (val_query_camId[i]==1 and (2 not in g_camId)) or (val_query_camId[i]==2 and (1 not in g_camId)):\n",
    "        val_gallery_feat = np.append(val_gallery_feat, val_query_feat[[i],:],axis=0)\n",
    "        val_gallery_labels = np.append(val_gallery_labels, val_query_labels[i])\n",
    "        val_gallery_camId = np.append(val_gallery_camId, val_query_camId[i])\n",
    "        idxs_delete.append(i)\n",
    "\n",
    "val_query_feat = np.delete(val_query_feat, idxs_delete, axis = 0)\n",
    "val_query_labels = np.delete(val_query_labels, idxs_delete)\n",
    "val_query_camId = np.delete(val_query_camId, idxs_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluation function for testing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN (t_query_feat, t_gallery_feat):\n",
    "    # k nearest neighbours implementation\n",
    "    k = 10\n",
    "    top_k_rank = np.zeros((query_labels.shape[0],k))\n",
    "    top_k_precision = np.zeros((query_labels.shape[0],k))\n",
    "    top_k_recall = np.zeros((query_labels.shape[0],k))\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=20).fit(t_gallery_feat)\n",
    "    distances, indices = nbrs.kneighbors(t_query_feat)\n",
    "    # Indices contains indices of the closes gallery pictures to each query image\n",
    "\n",
    "    # Select the images where camId and label not the same as of the query image\n",
    "    # Take top k of them\n",
    "\n",
    "    for i in range (0, query_labels.shape[0], 1):\n",
    "        selected_indices = np.logical_not(np.logical_and(gallery_camId[indices[i, :]] == query_camId[i], gallery_labels[indices[i,:]] == query_labels[i]))\n",
    "        number_of_removed = np.sum(np.logical_and(gallery_camId == query_camId[i], gallery_labels == query_labels[i]))\n",
    "        removed_indices = indices [i, selected_indices]\n",
    "        is_same_label = (query_labels[i] == gallery_labels[removed_indices])\n",
    "\n",
    "        for j in range (0, k, 1):\n",
    "            top_k_rank[i,j] = np.sum(is_same_label[:(j+1)]) != 0\n",
    "            top_k_precision[i,j]=np.sum(is_same_label[:(j+1)])/(j+1)\n",
    "            top_k_recall[i,j] = np.sum(is_same_label[:(j+1)])/(np.sum(gallery_labels==query_labels[i])-number_of_removed)\n",
    "    \n",
    "    # calculate mAP\n",
    "    average_precisions = np.zeros((query_labels.shape))\n",
    "    for j in range (0, query_labels.shape[0], 1):\n",
    "        recall = top_k_recall[j, :]\n",
    "        precision = top_k_precision[j, :]\n",
    "        index = (recall).argsort()[::-1] # argsort returns the indices that would sort an array (in this case the vector recall)\n",
    "        recall = recall[index]\n",
    "        precision = precision[index]\n",
    "        recall_range = np.arange(0, 1.1, 0.1)\n",
    "        precision_range = np.zeros((recall_range.shape))\n",
    "        for i in range (0, recall_range.shape[0], 1):    \n",
    "            if (precision[recall>=recall_range[i]].size != 0):\n",
    "                precision_range[i] = np.max(precision[recall>=recall_range[i]])\n",
    "            else:     \n",
    "                precision_range[i] = 0\n",
    "        average_precisions[j] = np.mean(precision_range)\n",
    "    \n",
    "    # print results\n",
    "    print(\"Mean average precision:\", average_precisions.mean())\n",
    "    print(\"Top1:\", top_k_rank[:, 0].mean())\n",
    "    print(\"Top5:\", top_k_rank[:, 4].mean())\n",
    "    print(\"Top10:\", top_k_rank[:, 9].mean())\n",
    "    print(\"Top5-Precision:\", top_k_precision[:, 4].mean())\n",
    "    print(\"Top5-Recall:\", top_k_recall[:, 4].mean())\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluation function for validation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_val (t_query_feat, t_gallery_feat):\n",
    "    # k nearest neighbours implementation\n",
    "    k = 10\n",
    "    top_k_rank = np.zeros((val_query_labels.shape[0],k))\n",
    "    top_k_precision = np.zeros((val_query_labels.shape[0],k))\n",
    "    top_k_recall = np.zeros((val_query_labels.shape[0],k))\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=20).fit(t_gallery_feat)\n",
    "    distances, indices = nbrs.kneighbors(t_query_feat)\n",
    "    # Indices contains indices of the closes gallery pictures to each query image\n",
    "\n",
    "    # Select the images where camId and label not the same as of the query image\n",
    "    # Take top k of them\n",
    "\n",
    "    for i in range (0, val_query_labels.shape[0], 1):\n",
    "        selected_indices = np.logical_not(np.logical_and(val_gallery_camId[indices[i, :]] == val_query_camId[i], val_gallery_labels[indices[i,:]] == val_query_labels[i]))\n",
    "        number_of_removed = np.sum(np.logical_and(val_gallery_camId == val_query_camId[i], val_gallery_labels == val_query_labels[i]))\n",
    "        removed_indices = indices [i, selected_indices]\n",
    "        is_same_label = (val_query_labels[i] == val_gallery_labels[removed_indices])\n",
    "\n",
    "        for j in range (0, k, 1):\n",
    "            top_k_rank[i,j] = np.sum(is_same_label[:(j+1)]) != 0\n",
    "            top_k_precision[i,j]=np.sum(is_same_label[:(j+1)])/(j+1)\n",
    "            top_k_recall[i,j] = np.sum(is_same_label[:(j+1)])/(np.sum(val_gallery_labels==val_query_labels[i])-number_of_removed)\n",
    "\n",
    "    # calculate mAP\n",
    "    average_precisions = np.zeros((val_query_labels.shape))\n",
    "    for j in range (0, val_query_labels.shape[0], 1):\n",
    "        recall = top_k_recall[j, :]\n",
    "        precision = top_k_precision[j, :]\n",
    "        index = (recall).argsort()[::-1] # argsort returns the indices that would sort an array (in this case the vector recall)\n",
    "        recall = recall[index]\n",
    "        precision = precision[index]\n",
    "        recall_range = np.arange(0, 1.1, 0.1)\n",
    "        precision_range = np.zeros((recall_range.shape))\n",
    "        for i in range (0, recall_range.shape[0], 1):    \n",
    "            if (precision[recall>=recall_range[i]].size != 0):\n",
    "                precision_range[i] = np.max(precision[recall>=recall_range[i]])\n",
    "            else:     \n",
    "                precision_range[i] = 0\n",
    "        average_precisions[j] = np.mean(precision_range)\n",
    "    \n",
    "    # print results\n",
    "    print(\"Mean average precision:\", average_precisions.mean())\n",
    "    print(\"Top1:\", top_k_rank[:, 0].mean())\n",
    "    print(\"Top5:\", top_k_rank[:, 4].mean())\n",
    "    print(\"Top10:\", top_k_rank[:, 9].mean())\n",
    "    print(\"Top5-Precision:\", top_k_precision[:, 4].mean())\n",
    "    print(\"Top5-Recall:\", top_k_recall[:, 4].mean())\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO KMEANS AND CMC -> INCLUDE THEM IN THE EVALUATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans clustering\n",
    "\n",
    "# very long \n",
    "#k_means_predictions = np.zeros((query_labels.shape[0]))\n",
    "#for i in range (0, query_labels.shape[0], 1):\n",
    "#    gallery_feat_removed, gallery_labels_removed = get_removed_gallery (i)\n",
    "#    kmeans = KMeans(n_clusters=((np.unique(gallery_labels_removed)).shape[0]), n_init=1, max_iter=10, random_state=0, precompute_distances= True).fit(gallery_feat_removed)\n",
    "#    k_means_predictions[i] = kmeans.predict(query_feat[i:(i+1), :])\n",
    "#    print (i)\n",
    "#    print (k_means_predictions[i], query_labels[i])\n",
    "#accuracy = (np.sum(k_means_predictions == query_labels))/(query_labels.shape[0])\n",
    "#print (accuracy)\n",
    "#print(k_means_predictions, k_means_predictions.shape, query_labels, query_labels.shape)\n",
    "\n",
    "import sklearn.utils.linear_assignment_ as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "def best_map(l1, l2):\n",
    "    \"\"\"\n",
    "    Permute labels of l2 to match l1 as much as possible\n",
    "    \"\"\"\n",
    "    if len(l1) != len(l2):\n",
    "        print(\"L1.shape must == L2.shape\")\n",
    "        exit(0)\n",
    "\n",
    "    label1 = np.unique(l1)\n",
    "    n_class1 = len(label1)\n",
    "\n",
    "    label2 = np.unique(l2)\n",
    "    n_class2 = len(label2)\n",
    "\n",
    "    n_class = max(n_class1, n_class2)\n",
    "    G = np.zeros((n_class, n_class))\n",
    "\n",
    "    for i in range(0, n_class1):\n",
    "        for j in range(0, n_class2):\n",
    "            ss = l1 == label1[i]\n",
    "            tt = l2 == label2[j]\n",
    "            G[i, j] = np.count_nonzero(ss & tt)\n",
    "\n",
    "    A = la.linear_assignment(-G)\n",
    "\n",
    "    new_l2 = np.zeros(l2.shape)\n",
    "    for i in range(0, n_class2):\n",
    "        new_l2[l2 == label2[A[i][1]]] = label1[A[i][0]]\n",
    "    return new_l2.astype(int)\n",
    "\n",
    "k_means_predictions = np.zeros((query_labels.shape[0]))\n",
    "\n",
    "kmeans = KMeans (n_clusters=np.unique(gallery_labels).shape[0], n_init =1, max_iter = 10, random_state = 0, precompute_distances=True).fit(gallery_feat)\n",
    "\n",
    "labels = best_map(gallery_labels, kmeans.labels_)\n",
    "\n",
    "#for i in range (0, query_labels.shape[0], 1):\n",
    "#    k_means_predictions[i] = kmeans.predict(query_feat[i:(i+1), :])\n",
    "#    print (i)\n",
    "#    print (k_means_predictions[i], query_labels[i])\n",
    "#accuracy = (np.sum(k_means_predictions == labels))/(query_labels.shape[0])\n",
    "#print (accuracy)\n",
    "#print(k_means_predictions, k_means_predictions.shape, query_labels, query_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1426   51   51 ... 1463 1463 1463] [   3    3    3 ... 1463 1463 1463] [582  63  63 ... 178 178 178]\n",
      "(5328,) (5328,) (5328,)\n"
     ]
    }
   ],
   "source": [
    "print (labels, gallery_labels, kmeans.labels_)\n",
    "print (labels.shape, gallery_labels.shape, kmeans.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3604"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels==gallery_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluation of original features </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 0.4353663677592249\n",
      "Top1: 0.47\n",
      "Top5: 0.6685714285714286\n",
      "Top10: 0.7492857142857143\n",
      "Top5-Precision: 0.3425714285714286\n",
      "Top5-Recall: 0.4498214285714286\n"
     ]
    }
   ],
   "source": [
    "kNN(query_feat, gallery_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluation of validation set with original features </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "kNN_val(val_query_feat, val_gallery_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> PCA </b> <br>\n",
    "Plot the eigenvalues of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAI/CAYAAACMOSbWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzs3XmcHVWZ//HPt/fOnpBmSYCEYEQWh4ARUBxZVBZHBHdxYdEBZhQVd9SfCi6jzqgMo+OCgqKCggiKiAgji6KyBAmQEED2BEJIIPvSSXc/vz/qXHJzud19u5N7q7vv9/163VdX1TlV9dyqdPLknFN1FBGYmZmZ2WYNeQdgZmZmNtQ4QTIzMzMr4QTJzMzMrIQTJDMzM7MSTpDMzMzMSjhBMjMzMyvhBMnqiqR3Sro27zj6IulGSf+adxwFkvaQdKek1ZI+WOE+IekF1Y6tViT9XtKJeceRJ0mflvTDvOMwqxX5PUg20kh6FNgB6C7a/OOIOD2fiAZG0o3AzyJiSPxjJOl8YFVEfLiX8hspiVdSADMj4sHaRGmDJelQsvu3c96xmA0lTXkHYFYlx0TE/+UdxAgxDfhF3kHkQZLI/iPZk3cseZLUFBFdecdhVkvuYrO6IukkSTcXrR8h6X5JKyV9R9JNxd1bkt4jaYGk5ZL+IGlaUVlI+jdJ/0jl/6tMq6QVkvYpqtshab2k7SVNlHSVpKVpv6sklf3fu6SzJP2saH16Om9TWh8v6XxJiyU9IelLkhpT2QvS91kpaZmkS/q4Lq+XND/FfaOkPdP264HDgG9LWiPphSX7fRn456LybxcVv7r02lRyXcvE9gpJf02xLZR0UtF3/0m6jo9J+n+SGrb2+qfv/2VJfwHWATOKuz0l7S7peknPpOt6kaQJRfs/Kuljku5O1/4SSW1F5cdKmitplaSHJB3V370sc03OkvRLST9T1vV5j6QXSvqUpKfTdTqiqP7J6XqvlvSwpNPS9tHA74Ep6f6tkTQlHf+ydPxVwEnFfxYlvS0dZ1xaP1rSU5I6eruPZsONEySrW5ImA5cBnwK2A+4HXl5UfhzwaeCNQAfwZ+DnJYd5HfBSYF/grcCREdEJXA4cX1TvrcBNEfE02e/dj8haZnYF1gPFicVAXAh0AS8A9gOOAAoJ3heBa4GJwM7At8odICU9PwfOSN/zauC3kloi4vD0vU+PiDER8UDxvhHxmZLy4m7M512bdL5Krmshtl3J/gH/Vqo7C5ibir8FjAdmAIcAJwAnb6Pr/27gVGAs8FhpWMBXgCnAnsAuwFkldd4KHAXsBvwTcFL6PgcAPwE+DkwAXgk8mvbp616WcwzwU7L7eyfwh/TdpgJfAL5fVPdpsvsxDjgZOEfS/hGxFjgaeDLdvzER8WTa51iy348JwEXFJ46IS4C/Af8jaTvgfOBfI2JpH/GaDS8R4Y8/I+pD9g/OGmBF0eeUVHYScHNaPgH4W9F+AhaS/UUP2T/M7y0qbyBrUZiW1gN4RVH5pcCZafnVwMNFZX8BTugl3lnA8qL1G4tiOItsfEihbHo6bxPZOKtOoL2o/HjghrT8E+A8YOd+rtdngUtLvucTwKGl8fSy//PK+7k2fV7XkuN8CriizPbG9N33Ktp2GnDjNrr+X+jvOxaVHQfcWfLn711F6/8JfC8tfx84p8wx+ryXZeqfBVxXtH4M2Z/5xrQ+Nt2DCb3s/2vgQ2n5UGBRmeP/qcy24j+LE4DHgXuA7w/md9Uff4byxy1INlIdFxETij4/KFNnCllCBEBEBLCoqHwacG7qrlkBPEuWRE0tqvNU0fI6YExavh5ol3Rg6j6aBVwBIGmUpO+nbqFVwJ+ACb11p/RhGtAMLC6K8fvA9qn8Eyne21L32Xt6Oc4UilpJIhtvs7Dkew5Gb9emkutasAvwUJntk4EWtmzdeazoGFt7/RfSi9RN94vUDbYK+FmKp5Lv3tv36e9elrOkaHk9sCwiuovWKZw3dYHdIunZdOzXlom5VK/XACAiVgC/BPYBvtHPscyGHQ/Stnq2mKzrCXhuQG7xWKCFwJcj4qLSHfsTET2SLiVrBVgCXBURq1PxR4E9gAMj4ilJs8i6SFTmUGuBUUXrO5bE1wlMjjIDaCPiKeCU9N1eAfyfpD/F858sexJ4cWElXYddyFqRKvq6FdYrjrvS67oQOKDM9mXAJrLE4t60bVdSzNvg+vf1nb6Syv8pIp5JXYaVdpEuBHbvZXuv93JrSGoFfkXWYvqbiNgk6dds/r69fdc+72u6bu8h6x79H7IuRbMRwy1IVs9+B7xY0nHKBj2/ny0TkO8Bn5K0Nzw3iPYtAzj+xcDbgHem5YKxZP/DXyFpEvD5Po4xF3ilpF0ljSfrcgIgIhaTjTH6hqRxygYo7y7pkBTvW7R58PFysn/wunm+S4F/kfQqSc1kCUQn8NcKv+cSsnFAlRrIdb2IbLD3WyU1SdpO0qzUUnIp8GVJY1Mr0UfIWnMKtsX1L2csqQtX0lSy8USVOh84OV3rBklTJb2ov3u5lVqAVmAp0CXpaLLxTQVLgO3Sn6+KKBt0/jOysWQnA1MlvW8bxGo2ZDhBspHqt0VP5ayRdEVphYhYBryFbIzIM8BewByy5ICIuAL4GvCL1JUyj2xAa0Ui4layFqApZONuCv4baCdrBbkFuKaPY1wHXALcDdwBXFVS5QSyfwDvJUuCLgN2SmUvBW6VtAa4kmzMySNlznE/8C6yQc/LyMazHBMRGyv8qucCb1b2RNj/9Fd5INc1Ih4n6w76KFlX3FyyQd8AHyC7vg8DN5MlQRcU7bvV178XZwP7AyvJkuzLK90xIm4jDZJO+99E1goGfd/LQUstZx8kSyiXA+8g+/NQKL+PrBXo4dS9N6WCw36FbNzSdyMbFP8u4EuSZm5tvGZDhV8UaZZIaiAbg/TOiLgh73jMzCw/bkGyuibpSEkT0jiNT5ONy7gl57DMzCxnTpCs3r2M7KmiQtfScRGxvu9dzMxspHMXm5mZmVkJtyCZ2YBJ+rGkL6XlQyUt6m+fXo4z5GeIVzZ1yKvzjsPMassJkpn1StkcZMvTGK3BHiMkrS15qvATABHxHxHR13QaZma58IsizawsSdPJJqJdCbye7K3Jg7VvmRdUmpkNWW5BMrPenED2RN+PgROrcQIVzRCf1k9IU4A8I+mzxd1b6eWJZ0p6KJVfml70iKTpqaXqREmPS1om6TOpbIqk9YW6adt+qU5zeiHj9emYyyRdJGlCL/E+17WY1rfoXkzn+pWkpZIekfTBorIDJM2RtErSEknf3JbX0sy2LSdIZtabE8jeZH0RcKSkHap5Mkl7Ad8he/P1TsB4tpyf7YNkE8MeQvbyx+XA/5Yc5hVk04i8CvicpD0jm53+b8Cbiuq9A7gsIjaRvdrhK+mYe5JNs3LWIOJvAH4L3JXifhVwhqQjU5VzgXMjYhzZdCOXDvQcZlY7TpDM7HmUzd02Dbg0Iu4gexXCO7bikH9Pb2kufI4sU+fNwG8j4ub0Fu/PseV8YKcBn4mIRentzWeRvcG7eKjA2RGxPiLuIktUCm/dvphsXrbCXHNvT9uIiAcj4rqI6IyIpcA3yZKwgXop0BERX4iIjRHxMPCDdC7I5o57gaTJEbEmIvy+LbMhzAmSmZVzInBtmo4FsmRia7rZ9o+ICUWfP5SpM4WiGeQjYh3ZFDAF04ArtHm2+wVkc8sVt2w9VbS8jjSbPdm0HS9L02i8kizx+jOApO0l/ULSE2nqk5/R/0z35UwDphQngmQvHy3E917ghcB9km6X9LpBnMPMasSDtM1sC5LagbcCjZIKCUcrMEHSvql1phoWk3WPFcexXVH5QuA9EfGXMjFP7+vAEbFC0rVk32tP4Oex+SVwXyFLmP4pIp6RdBzw7V4OtRYYVbRePLnxQuCRiCg7H1lE/AM4PnXFvRG4TNJ2EbG2r9jNLB9uQTKzUseRtczsBcxKnz3JWlxOqOJ5LwOOkfRySS1kk8KqqPx7wJclTQOQ1CHp2AEc/2Ky+N+UlgvGAmuAFZKmAh/v4xhzgddKmiRpR+CMorLbgFWSPimpXVKjpH0kvTTF+y5JHRHRA6xI+3QPIH4zqyEnSGZW6kTgRxHxeEQ8VfiQtaq8s2TMT6XuKnkP0n+XVoiI+cAHgF+QtSatBp4GOlOVc8lmob9W0mqyJ+wOHEAMVwIzgSUlrWBnA/uTvc7gd8DlfRzjp2Rjmx4FrgUuKYq/m2y6mlnAI2TT1/yQbLA5wFHAfElr0nd5e0RsGED8ZlZDnmrEzIYkSWPIWlpmRsQjecdjZvXFLUhmNmRIOkbSKEmjga8D95C11piZ1ZQTJDMbSo4FnkyfmWTdUG7mNrOacxebmZmZWQm3IJmZmZmVGBbvQZo8eXJMnz497zDMzMxsmLvjjjuWRURHf/WGRYI0ffp05syZk3cYZmZmNsxJeqySeu5iMzMzMyvhBMnMzMyshBMkMzMzsxJOkMzMzMxKOEEyMzMzK+EEyczMzKyEEyQzMzOzElVLkCS1SbpN0l2S5ks6O23fTdKtkv4h6RJJLdWKoRJfu+Y+jjznT3mGYGZmZkNMNVuQOoHDI2JfYBZwlKSDgK8B50TETGA58N4qxtCvNRu6WLqmM88QzMzMbIipWoIUmTVptTl9AjgcuCxtvxA4rloxVKJB0OMJe83MzKxIVccgSWqUNBd4GrgOeAhYERFdqcoiYGo1Y+iPJHp6nCCZmZnZZlVNkCKiOyJmATsDBwB7lqtWbl9Jp0qaI2nO0qVLqxZjg4QbkMzMzKxYTZ5ii4gVwI3AQcAESYVJcncGnuxln/MiYnZEzO7o6HfS3UFzF5uZmZmVquZTbB2SJqTlduDVwALgBuDNqdqJwG+qFUMlGhqEe9jMzMysWFP/VQZtJ+BCSY1kidilEXGVpHuBX0j6EnAncH4VY+iXcAuSmZmZbalqCVJE3A3sV2b7w2TjkYYESeUHQZmZmVndqvs3aTcIwi1IZmZmVsQJkjwGyczMzLbkBMlPsZmZmVmJuk+QlN6D5G42MzMzK6j7BKlBAvDLIs3MzOw5TpCy/MjdbGZmZvYcJ0gpQ/JAbTMzMyuo+wSpwC1IZmZmVlD3CZLHIJmZmVkpJ0hpDFL4fdpmZmaWOEGSxyCZmZnZluo+QZKfYjMzM7MSdZ8gPTcGqSfnQMzMzGzIcILkFiQzMzMr4QTpufcgOUEyMzOzTN0nSPIgbTMzMytR9wnSc4/5uwXJzMzMkrpPkIRbkMzMzGxLdZ8g+UWRZmZmVsoJkscgmZmZWYm6T5Cee1GkMyQzMzNL6j5B8mS1ZmZmVsoJUroCfg+SmZmZFThBSi1I3U6QzMzMLKlagiRpF0k3SFogab6kD6XtZ0l6QtLc9HlttWKoRFNqQurqdoJkZmZmmaYqHrsL+GhE/F3SWOAOSdelsnMi4utVPHfFWpuyBGljl2erNTMzs0zVEqSIWAwsTsurJS0AplbrfIPVkhKkzq7unCMxMzOzoaImY5AkTQf2A25Nm06XdLekCyRNrEUMvXELkpmZmZWqeoIkaQzwK+CMiFgFfBfYHZhF1sL0jV72O1XSHElzli5dWrX4NrcgOUEyMzOzTFUTJEnNZMnRRRFxOUBELImI7ojoAX4AHFBu34g4LyJmR8Tsjo6OqsXY2tQIOEEyMzOzzar5FJuA84EFEfHNou07FVV7AzCvWjFUorXZY5DMzMxsS9V8iu1g4N3APZLmpm2fBo6XNAsI4FHgtCrG0K+WRnexmZmZ2Zaq+RTbzYDKFF1drXMOxrj2ZgBWrd+UcyRmZmY2VNT9m7THtTXR3CiWrdmYdyhmZmY2RNR9giSJSaNbeHZtZ96hmJmZ2RBR9wkSZE+ybfJUI2ZmZpY4QQKaGkRXjxMkMzMzyzhBAhobRHePn2IzMzOzjBMksgSpy11sZmZmljhBApoaRbe72MzMzCxxggQ0ymOQzMzMbDMnSBTGIDlBMjMzs4wTJKCpoYEuD9I2MzOzxAkSbkEyMzOzLTlBwoO0zczMbEtOkHALkpmZmW3JCRJ+k7aZmZltyQkSbkEyMzOzLfWbIEnaQdL5kn6f1veS9N7qh1Y72VNsTpDMzMwsU0kL0o+BPwBT0voDwBnVCigPbkEyMzOzYpUkSJMj4lKgByAiuoDuqkZVY02NYmOX34NkZmZmmUoSpLWStgMCQNJBwMqqRlVjrU2NdDpBMjMzs6SpgjofAa4Edpf0F6ADeHNVo6qxtuYGOjeNqEYxMzMz2wr9JkgR8XdJhwB7AALuj4hNVY+shtyCZGZmZsUqeYrt/cCYiJgfEfOAMZLeV/3QaqetuYGN3T30eKC2mZmZUdkYpFMiYkVhJSKWA6dUL6Taa21qBHArkpmZmQGVJUgNklRYkdQItFQvpNprbcouQ2eXxyGZmZlZZYO0/wBcKul7ZE+y/RtwTVWjqrG25qwFacMmtyCZmZlZZS1InwSuB/4deD/wR+AT/e0kaRdJN0haIGm+pA+l7ZMkXSfpH+nnxK35AtuCW5DMzMysWCVPsfUA302fgegCPpqeghsL3CHpOuAk4I8R8VVJZwJnkiVhuSm0IHkMkpmZmUFlT7EdnFp6HpD0sKRHJD3c334RsTgi/p6WVwMLgKnAscCFqdqFwHGDD3/bKLQgbfC7kMzMzIzKxiCdD3wYuINBTjEiaTqwH3ArsENELIYsiZK0fS/7nAqcCrDrrrsO5rQVa20udLG5BcnMzMwqS5BWRsTvB3sCSWOAXwFnRMSqogfi+hQR5wHnAcyePbuqLyjaPEjbLUhmZmZWWYJ0g6T/Ai4HOgsbC91nfZHUTJYcXRQRl6fNSyTtlFqPdgKeHkTc29Rzg7T9FJuZmZlRWYJ0YPo5u2hbAIf3tVN6d9L5wIKI+GZR0ZXAicBX08/fVBxtlXiQtpmZmRWr5Cm2wwZ57IOBdwP3SJqbtn2aLDG6VNJ7gceBtwzy+NuMB2mbmZlZsUpakJD0L8DeQFthW0R8oa99IuJmsslty3lVpQHWgqcaMTMzs2KVPOb/PeBtwAfIEp63ANOqHFdNtTW7BcnMzMw2q+RN2i+PiBOA5RFxNvAyYJfqhlVbbkEyMzOzYpUkSOvTz3WSpgCbgN2qF1LteaoRMzMzK1bJGKSrJE0A/gv4O9kTbD+salQ11tAgWhobPFmtmZmZAZU9xfbFtPgrSVcBbRGxsrph1V5rU4NbkMzMzAzoI0GSdHhEXC/pjWXKKHrx44jQ2tzoFiQzMzMD+m5BOgS4HjimTFmQvVl7xHALkpmZmRX0miBFxOclNQC/j4hLaxhTLlqbG/wUm5mZmQH9PMUWET3A6TWKJVdtTY10+j1IZmZmRmWP+V8n6WOSdpE0qfCpemQ15hYkMzMzK6jkMf/3pJ/vL9oWwIxtH05+2poa/SZtMzMzAyp7zH9EvRSyN63NDaxd25V3GGZmZjYEVDpZ7T7AXmw5We1PqhVUHlqbGuj0Y/5mZmZGBQmSpM8Dh5IlSFcDRwM3AyMqQWprbmSDH/M3MzMzKhuk/WbgVcBTEXEysC/QWtWocuAWJDMzMyuoaLLa9Lh/l6RxwNOMsAHa4BYkMzMz26ySMUhz0mS1PwDuANYAt1U1qhy4BcnMzMwKKnmK7X1p8XuSrgHGRcTd1Q2r9lqbshakiEBS3uGYmZlZjnrtYpN0r6TPSNq9sC0iHh2JyRFAW3MDEbCpO/IOxczMzHLW1xik44ExwLWSbpV0hqQpNYqr5lqbGgE8Ya2ZmZn1niBFxF0R8amI2B34EDANuEXS9ZJOqVmENdLWnF2KDR6HZGZmVvcqeYqNiLglIj4MnABMBL5d1ahy4BYkMzMzK6jkRZEvJetuexPwKHAe8MvqhlV7rW5BMjMzs6TXBEnSfwBvA5YDvwAOjohFtQqs1tyCZGZmZgV9dbF1AkdHxOyI+PpAkyNJF0h6WtK8om1nSXpC0tz0ee1gA9/WCi1InV1uQTIzM6t3fQ3SPjsiHtiKY/8YOKrM9nMiYlb6XL0Vx9+m2lIL0oZNbkEyMzOrdxUN0h6MiPgT8Gy1jr+tjWrJEqR1nU6QzMzM6l3VEqQ+nC7p7tQFN7G3SpJOlTRH0pylS5dWPajJY7P5d5et6az6uczMzGxo6+tN2vv39Rnk+b4L7A7MAhYD3+itYkScl8Y/ze7o6Bjk6So3eUwLAE+vdoJkZmZW7/p6zL+QvLQBs4G7AAH/BNwKvGKgJ4uIJYVlST8ArhroMaqltamR9uZGVm/YlHcoZmZmlrO+BmkfFhGHAY8B+6fWnJcA+wEPDuZkknYqWn0DMK+3unkY1dLIuo0eg2RmZlbv+n1RJPCiiLinsBIR8yTN6m8nST8HDgUmS1oEfB44NO0bZC+dPG0wQVdLe0sj650gmZmZ1b1KEqQFkn4I/IwssXkXsKC/nSLi+DKbzx9YeLXlFiQzMzODyhKkk4F/J5uwFuBPZIOtR5z2libW+T1IZmZmda/fBCkiNkj6HnB1RNxfg5hyM6q5kfUbu/IOw8zMzHLW73uQJL0emAtck9ZnSbqy2oHlwV1sZmZmBpW9KPLzwAHACoCImAtMr2JMufEgbTMzM4PKEqSuiFhZ9UiGALcgmZmZGVQ2SHuepHcAjZJmAh8E/lrdsPIxqqWJdR6DZGZmVvcqaUH6ALA30AlcDKwEzqhmUHlpb2lkvZ9iMzMzq3t9tiBJagTOjoiPA5+pTUj5GdXcyKbuYFN3D82Neczja2ZmZkNBn1lARHQDL6lRLLlrb2kE8DgkMzOzOlfJGKQ702P9vwTWFjZGxOVViyono1qyy7F+Yzfj25tzjsbMzMzyUkmCNAl4Bji8aFsAIzBBKrQgeaC2mZlZPavkTdon1yKQocBdbGZmZgYVJEiSfkTWYrSFiHhPVSLKUaEFyU+ymZmZ1bdKutiuKlpuA94APFmdcPI1yi1IZmZmRmVdbL8qXpf0c+D/qhZRjtqbC4O0PQbJzMysng3mZT8zgV23dSBDQaEFaW2nW5DMzMzqWSVjkFaz5Rikp4BPVi2iHD3XxeYxSGZmZnWtki62sbUIZCgY1ZpdjnWd7mIzMzOrZ/12sUn6YyXbRoLRLY2MbWti4fJ1eYdiZmZmOeq1BUlSGzAKmCxpIqBUNA6YUoPYak4S07YbxRPL1+cdipmZmeWory6204AzyJKhvxdtXwX8bzWDytPolibW+jF/MzOzutZrghQR5wLnSvpARHyrhjHlanRrE0tXd+YdhpmZmeWokhdF/lDSR4BXkD3N9mfgexGxoaqR5aS9pZG1fg+SmZlZXaskQboQWA0UWpGOB34KvKVaQeVpdEsj693FZmZmVtcqSZD2iIh9i9ZvkHRXfztJugB4HfB0ROyTtk0CLgGmA48Cb42I5QMNuppGtTSx1o/5m5mZ1bVK3qR9p6SDCiuSDgT+UsF+PwaOKtl2JvDHiJgJ/DGtDyntLY2erNbMzKzOVZIgHQj8VdKjkh4F/gYcIukeSXf3tlNE/Al4tmTzsWRddqSfxw085Ooa3dLIpu5gY1dP3qGYmZlZTirpYittBdoaO0TEYoCIWCxp+94qSjoVOBVg111rN/Vbe0thwtpuWpoGM1WdmZmZDXf9ZgAR8VhEPAasJ3uKLbLNz22viog4LyJmR8Tsjo6Oap3meUYXJqz1k2xmZmZ1q5KpRl4v6R/AI8BNZIOrfz/I8y2RtFM67k7A04M8TtW0Fyas9ZNsZmZmdauSPqQvAgcBD0TEbsCrqGyQdjlXAiem5ROB3wzyOFUzOnWxrXMLkpmZWd2qJEHaFBHPAA2SGiLiBmBWfztJ+jnZgO49JC2S9F7gq8BrUovUa9L6kDLKLUhmZmZ1r5JB2iskjQH+BFwk6Wmg3+aViDi+l6JXDSC+mit0sfllkWZmZvWrkhakY4F1wIeBa4CHgGOqGVSeRrdmOaMHaZuZmdWvXhMkSS+QdHBErI2InojoiogLgbnAhNqFWFvtze5iMzMzq3d9tSD9N9kcbKXWpbIRqdCCtM7TjZiZmdWtvhKk6RHxvDdlR8QcsrnURqTnBml7uhEzM7O61VeC1NZHWfu2DmSoaG1qoEGwrtMJkpmZWb3qK0G6XdIppRvT4/p3VC+kfEliVEuTxyCZmZnVsb4e8z8DuELSO9mcEM0GWoA3VDuwPLW3NPpFkWZmZnWs1wQpIpYAL5d0GLBP2vy7iLi+JpHlaFxbE8vWbMw7DDMzM8tJvy+KTG/OvqEGsQwZL546ntseeTbvMMzMzCwnlbwosu5MHtPK8nWb8g7DzMzMcuIEqYzx7c2s39TNxq6evEMxMzOzHDhBKmP8qGYAVm1wK5KZmVk9coJUxri2LEFaud4JkpmZWT1yglTG+HYnSGZmZvXMCVIZ41KCtMoJkpmZWV1yglSGW5DMzMzqmxOkMsa1Z6+HcguSmZlZfXKCVEahBWnVBk83YmZmVo+cIJXR2tRIW3ODu9jMzMzqlBOkXkyd0M7cx1fkHYaZmZnlwAlSL162+3bcv2R13mGYmZlZDpwg9WLqhFGsXL+JNZ0eh2RmZlZvnCD1YurEdgCeWL4+50jMzMys1pwg9WLqhJQgrViXcyRmZmZWa015nFTSo8BqoBvoiojZecTRl53dgmRmZla3ckmQksMiYlmO5+9Tx5hWmhvFkys35B2KmZmZ1Zi72HrR0CDGtTX7XUhmZmZ1KK8EKYBrJd0h6dRyFSSdKmmOpDlLly6tcXiZsW1NrPHbtM3MzOpOXgnSwRGxP3A08H5JryytEBHnRcTsiJjd0dFR+wiBMW1NfszfzMysDuWSIEXEk+nn08AVwAF5xNGfsa3NnrDWzMysDtU8QZI0WtLYwjJwBDCv1nFUYueJ7Tz2rB/zNzMzqzd5tCDtANws6S7gNuB3EXFNDnH0a7eO0Sxd3cm6je5mMzMzqyc1f8w/Ih4G9q31eQdj+7FtACxd3cm07fJ8I4KZmZnVkh/z78P2Y1sBWOx3IZmZmdUVJ0h9eNGOYwGY/+SqnCMxMzOzWnKC1Iftx7UxdUI7dz6+PO9QzMzMrIacIPVjjx3H8tDStXmHYWZmZjXkBKkfu04axePPrCUi8g7FzMzMasQJUj+mbTdXBhVSAAAgAElEQVSKtRu7eWbtxrxDMTMzsxpxgtSP3SaPBuCBJatzjsTMzMxqxQlSP/bbZSIAcxeuyDkSMzMzqxUnSP0YP6qZHca1cq8f9TczM6sbTpAq8Oo9d+Da+UtY7nFIZmZmdcEJUgWO3mcnNnb3sOAptyKZmZnVAydIFdh9+2ygtt+HZGZmVh+cIFVgx3FtjGpp5PZHns07FDMzM6sBJ0gVkMTr/mknfj9vMc+s6cw7HDMzM6syJ0gVeu8rZrCpO/j13CfzDsXMzMyqzAlShfbYcSz77jyeS29f6GlHzMzMRjgnSAPw1pfuwv1LVnP3opV5h2JmZmZV5ARpAI7ZdwqjWhr51vUP5h2KmZmZVZETpAEY19bMaa/cnf9bsIQ7HluedzhmZmZWJU6QBugdB+7KzhPbeff5t/LXB5flHY6ZmZlVgROkAeoY28rl//5ypk5o590X3MaFf32Uru6evMMyMzOzbcgJ0iBsP66Ni085iL2njOPzV87n5B/fzrwnPHDbzMxspHCCNEgdY1v59fsO5lNHv4g5jy7ndd+6mTd9969cceciOru68w7PzMzMtoKGwzt9Zs+eHXPmzMk7jF6tXLeJX96xkItufZxHlq2lvbmRg2ZMYvb0Sbxwh7FM224Uu04aRVtzY96hmpmZ1TVJd0TE7H7r5ZEgSToKOBdoBH4YEV/tq/5QT5AKIoI//2MZ19/3NDc9sJRHlm05ue32Y1tTsjSaaduNYsfxbWw3uoWJo1uYOKqFiaOaGdPaRFOjG/bMzMyqYcgmSJIagQeA1wCLgNuB4yPi3t72GS4JUqmV6zfx8NI1PP7sOh5/Zh2PpZ+PP7uOp1Zt6HW/US2NjG5tor25kfbmRtqaG2hrbqS9pZG2pkZamxtobWqgtamRlqYGmhpFc0P62dhAU4NoamyguVE0PbddNDY00CBokBDZHHOF9YaGwvrmbSqUpW1b1C8ub+jlmIU6DZUdUyp/PbIjl9neS/1K6w7kuL2dSmUq91638mOYmVl1VJogNdUimBIHAA9GxMMAkn4BHAv0miANV+Pbm9lv14nst+vE55Vt2NTN0tWdPLN2I8+u7WTl+k0sX7uJ1Ru6WL1hE2s3drFhUw/rN3azflP2Wb52I+s2drOxu4eNXT10dmU/N3X30NUTdPcM/e5S2zo1T97K7dFr3a07bu91yx23l+S2/CHKFgzoulUxtvLVt0XyXq7uyE/G6+H/GyP9K373XS9h310m5B1GLgnSVGBh0foi4MDSSpJOBU4F2HXXXWsTWQ21NTeyy6RR7DJp1DY7Zk9P0NUTWcLUHWzqST+LEqiIoCegJ4KeCCIgitZ7grJ1eoq2RQQ9PZStHxTWN9cpu39suX9vyV1vKV+5hs/opfZAGknLtaj2tn+5zb3XrTy2XsMtF1vlVQd8fbb6+w3ouFt/fQYS29b+megtjmre//LHHsBx6+D/T71d55GkHu7j2LY8UpPnyyOKcsnv8255RJwHnAdZF1u1gxoJGhpES4NoafIYJjMzs62Rx7+ki4BditZ3Bp7MIQ4zMzOzsvJIkG4HZkraTVIL8HbgyhziMDMzMyur5l1sEdEl6XTgD2SP+V8QEfNrHYeZmZlZb3IZCRURVwNX53FuMzMzs/54NK+ZmZlZCSdIZmZmZiWcIJmZmZmVcIJkZmZmViKXyWoHStJS4LEqnmIysKyKx7d8+L6OXL63I5Pv68g01O7rtIjo6K/SsEiQqk3SnEomrrPhxfd15PK9HZl8X0em4Xpf3cVmZmZmVsIJkpmZmVkJJ0iZ8/IOwKrC93Xk8r0dmXxfR6ZheV89BsnMzMyshFuQzMzMzEo4QTIzMzMrUfcJkqSjJN0v6UFJZ+Ydjw2MpEcl3SNprqQ5adskSddJ+kf6OTFtl6T/Sff6bkn75xu9FUi6QNLTkuYVbRvwfZR0Yqr/D0kn5vFdbEu93NuzJD2Rfm/nSnptUdmn0r29X9KRRdv9d/UQImkXSTdIWiBpvqQPpe0j5/c2Iur2AzQCDwEzgBbgLmCvvOPyZ0D38FFgcsm2/wTOTMtnAl9Ly68Ffg8IOAi4Ne/4/Xnunr0S2B+YN9j7CEwCHk4/J6bliXl/t3r/9HJvzwI+VqbuXunv4VZgt/T3c6P/rh56H2AnYP+0PBZ4IN2/EfN7W+8tSAcAD0bEwxGxEfgFcGzOMdnWOxa4MC1fCBxXtP0nkbkFmCBppzwCtC1FxJ+AZ0s2D/Q+HglcFxHPRsRy4DrgqOpHb33p5d725ljgFxHRGRGPAA+S/T3tv6uHmIhYHBF/T8urgQXAVEbQ7229J0hTgYVF64vSNhs+ArhW0h2STk3bdoiIxZD9EgPbp+2+38PLQO+j7+/wcnrqarmg0A2D7+2wJGk6sB9wKyPo97beEySV2eb3HgwvB0fE/sDRwPslvbKPur7fI0Nv99H3d/j4LrA7MAtYDHwjbfe9HWYkjQF+BZwREav6qlpm25C+t/WeIC0Cdila3xl4MqdYbBAi4sn082ngCrKm+CWFrrP08+lU3fd7eBnoffT9HSYiYklEdEdED/ADst9b8L0dViQ1kyVHF0XE5WnziPm9rfcE6XZgpqTdJLUAbweuzDkmq5Ck0ZLGFpaBI4B5ZPew8CTEicBv0vKVwAnpaYqDgJWFpmAbkgZ6H/8AHCFpYuqyOSJtsyGmZOzfG8h+byG7t2+X1CppN2AmcBv+u3rIkSTgfGBBRHyzqGjE/N425R1AniKiS9LpZDejEbggIubnHJZVbgfgiuz3lCbg4oi4RtLtwKWS3gs8Drwl1b+a7EmKB4F1wMm1D9nKkfRz4FBgsqRFwOeBrzKA+xgRz0r6Itk/pgBfiIhKBwdblfRybw+VNIusK+VR4DSAiJgv6VLgXqALeH9EdKfj+O/qoeVg4N3APZLmpm2fZgT93nqqETMzM7MS9d7FZmZmZvY8TpDMzMzMSjhBMjMzMyvhBMnMzMyshBMkMzMzsxJOkMwMAElrBlj/UElXVSuedI6fp+koPlxh/dcPhZneJf1Y0pvzjsPMBq+u34NkZkOXpB2Bl0fEtEr3iYgr8QsEzWwbcAuSmW0htQzdKOkySfdJuii9NRdJR6VtNwNvLNpndJp09HZJd0o6Nm3/iKQL0vKLJc2TNKrkfG2SfiTpnrTvYanoWmB7SXMl/XPJPh2SfpXOd7ukg9P2kyR9Oy3vLumWVP6F4hYySR9P2++WdHbaNl3SAkk/kDRf0rWS2iXtKem2on2nS7o7LX8uHWeepPMK16kk1kclTU7LsyXd2Nc1M7OhwQmSmZWzH3AGsBcwAzhYUhvZvFnHAP8M7FhU/zPA9RHxUuAw4L/S9C//DbxA0huAHwGnRcS6knO9HyAiXgwcD1yYzvV64KGImBURfy7Z51zgnHS+NwE/LPMdzgXOTXWem9tJ0hFkU1gcQDZZ6ku0eZLjmcD/RsTewArgTRGxAGiRNCPVeRtwaVr+dkS8NCL2AdqB15W7mL3o7ZqZ2RDgBMnMyrktIhalyUTnAtOBFwGPRMQ/InsF/8+K6h8BnJmmHLgRaAN2TfufBPwUuCki/lLmXK9I5UTEfcBjwAv7ie/VwLfT+a4ExinNy1fkZcAv0/LFJbEeAdwJ/D19r5mp7JGIKEybcEf63pAlRG9Ny28DLknLh0m6VdI9wOHA3v3EXazsNRvA/mZWRR6DZGbldBYtd7P574re5iYSWWvL/WXKZgJrgCl97DtQDcDLImL9Fgd6fg9Xb+f7SkR8v2Tf6Tz/e7en5UuAX0q6HIiI+Edq5foOMDsiFko6iyzJKdXF5v+MFpf3dc3MLGduQTKzSt0H7CZp97R+fFHZH4APFI1V2i/9HE/W1fVKYLtenuz6E/DOVP+FZK0o/SUN1wKnF1bSxKelbiHrfoNs9vfiWN8jaUzad6qk7fs6WUQ8RJYwfZbNrUeFZGdZOlZvT609CrwkLb+paHvZa2ZmQ4MTJDOrSERsAE4FfpcGaT9WVPxFoBm4W9K8tA5wDvCdiHgAeC/w1TLJyHeAxtRNdQlwUkR00rcPArPTIOt7gX8rU+cM4CNpgPVOwMr0Pa4l63L7WzrnZUBp91w5lwDvIo0/iogVZGOy7gF+zebZyEudDZwr6c9kSVZBb9fMzIYAZUMJzMxGlvS03PqICElvB46PCD8pZmYV8RgkMxupXkI2kFtkT6S9J+d4zGwYcQuSmZmZWQmPQTIzMzMr4QTJzMzMrIQTJDMzM7MSTpDMzMzMSjhBMjMzMyvhBMnMzMyshBMkMzMzsxJOkMzMzMxKOEEyMzMzK+EEyczMzKyEEyQzMzOzEk6QzKyuSPq9pBPzjqM3kqZLCkmeTNwsR06QzIYBSadLmiOpU9KPy5S/StJ9ktZJukHStKKyVkkXSFol6SlJH+njPCdJurlKX2ObkPSopFf3UX6opB5Ja0o+LwOIiKMj4sLaRWxmw5ETJLPh4UngS8AFpQWSJgOXA58FJgFzgEuKqpwFzASmAYcBn5B0VDWDHQKtH09GxJiSz99yjsnMhhEnSGbDQERcHhG/Bp4pU/xGYH5E/DIiNpAlRPtKelEqPwH4YkQsj4gFwA+AkwYag6STJS2QtFrSw5JOKyo7VNIiSZ+U9BTwo7T9E5IWS3pS0r+mrqMXpLJWSV+X9LikJZK+J6k9lU2WdJWkFZKelfRnSQ2SfgrsCvw2tQp9YhDf40ZJ/5qWGyV9Q9IySY+klrrnurckjZd0fvoOT0j6kqTGVHaSpJvTd1ie9j86lb1d0pyS835Y0pVp+V8k3Zla9RZKOquPeLdoMZN0lqSfFa0fJOmv6VrdJenQorKT0r1aneJ750Cvl1m9coJkNvztDdxVWImItcBDwN6SJgJTisvT8t6DOM/TwOuAccDJwDmS9i8q35GsBWsacGpqpfoI8GrgBcAhJcf7GvBCYFYqnwp8LpV9FFgEdAA7AJ/Ovlq8G3gcOCa1Cv3nIL5HsVOAo1MM+wPHlZRfCHSl+PYDjgD+taj8QOB+YDLwn8D5kgRcCewhaWZR3XcAF6fltWSJ6wTgX4B/l1R67n5Jmgr8jqx1cRLwMeBXkjokjQb+Bzg6IsYCLwfmDvQcZvXKCZLZ8DcGWFmybSUwNpVRUl4oG5CI+F1EPBSZm4BrgX8uqtIDfD4iOiNiPfBW4EcRMT8i1gFnFyqmJOIU4MMR8WxErAb+A3h7qrIJ2AmYFhGbIuLPEREDCHdKalEp/owuU++twLkRsSgilgNfLYpxB7Lk6YyIWBsRTwPnFMUI8FhE/CAiusmSqZ2AHdL3/Q1wfDrWTOBFZIkTEXFjRNwTET0RcTfwc56fQFbiXcDVEXF1OtZ1ZF2sr03lPcA+ktojYnFEzB/EOczqkhMks+FvDVmrTrFxwOpURkl5oWxAJB0t6ZbU5bWC7B/hyUVVlqYuvoIpwMKi9eLlDmAUcEchgQGuSdsB/gt4ELg2dRGdOcBwn4yICSWftWXq9RXjNKAZWFwU4/eB7YvqPFVYSEkRbE5KLyYlSGStR78u1JF0YBpMv1TSSuDf2PJaVmoa8JbiRBB4BbBT+r5vS8deLOl3Rd2uZtYPJ0hmw998YN/CSmop2Z1sXNJyYHFxeVoeUEuCpFbgV8DXyVpIJgBXAyqqVtrCsxjYuWh9l6LlZcB6YO+iBGZ8RIwBiIjVEfHRiJgBHAN8RNKrejnP1ugrxoVAJzC5KMZxEVFp9+S1wGRJs8gSpYuLyi4ma03aJSLGA99jy2tZbC1ZMlmwY0mMPy1JBEdHxFcBIuIPEfEaspat+8jGn5lZBZwgmQ0DkpoktQGNQKOktqInxa4g60Z5U6rzOeDuiLgvlf8E+H+SJqYWhFOAH/d9OrUVf4AWoBVYCnSlwchH9BP2pcDJkvaUNIrN44uIiB6yf6zPkbR9OulUSUem5ddJekHqilsFdKcPwBJgRj/nrtSlwIfSuScAnyyKcTFZkvMNSePSIPHdJVXUFRYRXcBlZK1hk4DriorHAs9GxAZJB5C1MPVmLvB2Sc2SZgNvLir7GXCMpCPTgPM2ZQPmd5a0g6TXp4S5k6w1sbvM8c2sDCdIZsPD/yNrcTmTbNzJ+rSNiFgKvAn4MrCcbOBw8TiZz5MN2n4MuAn4r4i4po9zvTwdv/TzQbKEYjnZP+hX9hVwRPyebJDwDWTdZYXH7DvTz0+m7bdIWgX8H7BHKpuZ1tek/b4TETemsq+QJXwrJH2sl9NP0fPfg/SmMvV+QJYE3Q3cSdYq1sXmROIEsuTw3vS9LyNrjanUxWSD1H+ZEqaC9wFfkLSaLHG8tI9jfJasRXA52Tiu51qiImIhcCzZIPalZC1KHyf7u72BbLD7k8CzZGOc3jeA2M3qmgY27tHMbHAk7QnMA1pLkoUhI7WMfS8ipvVb2cxGNLcgmVnVSHqDpJb0uoGvAb8dSsmRpHZJr01dmFPJWtuuyDsuM8ufEyQzq6bTyLp+HiLrtvr3fMN5HpF1Wy0n62JbQNFYKTOrX+5iMzMzMyvhFiQzMzOzEnlPKFmRyZMnx/Tp0/MOw8zMzIa5O+64Y1lEdPRXb1gkSNOnT2fOnDn9VzQzMzPrg6THKqnnLjYzMzOzEk6QzMzMzEo4QTIzMzMr4QTJzMzMrIQTJDMzM7MSTpDMzMzMStR9ghQRrOkcMlNDmZmZ2RBQtQRJUpuk2yTdJWm+pLPT9t0k3SrpH5IukdRSrRgq8Zlfz+PV37gpzxDMzMxsiKlmC1IncHhE7AvMAo6SdBDZjN7nRMRMsgki31vFGPo1ZXwbT63awFq3IpmZmVlStQQpMmvSanP6BHA4cFnafiFwXLViqMSMjjEAPLJsbZ5hmJmZ2RBS1TFIkholzQWeBq4DHgJWREShuWYRMLWXfU+VNEfSnKVLl1YtxhkdowF42AmSmZmZJVVNkCKiOyJmATsDBwB7lqvWy77nRcTsiJjd0dHvnHKDNn270Ujw8NI1/Vc2MzOzulCTp9giYgVwI3AQMEFSYZLcnYEnaxFDb9qaG5kyvt1dbGZmZvacaj7F1iFpQlpuB14NLABuAN6cqp0I/KZaMVRqRsdoHl7qBMnMzMwy1WxB2gm4QdLdwO3AdRFxFfBJ4COSHgS2A86vYgwVmTF5NI8sW0tE2d4+MzMzqzNN/VcZnIi4G9ivzPaHycYjDRkzOsawprOLpas72X5cW97hmJmZWc7q/k3asPlJtofczWZmZmY4QQJgt8lZguSB2mZmZgZOkACYMr6dtuYGP+pvZmZmgBMkABoaxPTtRvtlkWZmZgY4QXrOjI7R7mIzMzMzwAnSc2ZMHsPjz65jY1dP3qGYmZlZzpwgJTM6RtPdEzz+7Lq8QzEzM7OcOUFK/CSbmZmZFThBSmZ0jAE8aa2ZmZk5QXrO+PZmJo9p8ZxsZmZm5gSp2G6T/SSbmZmZOUHawozJY3h4mbvYzMzM6p0TpCIzOkazbM1GVq7flHcoZmZmliMnSEX8JJuZmZmBE6Qt+Ek2MzMzAydIW9h10igaG+Qn2czMzOqcE6QiLU0N7DKx3V1sZmZmdc4JUokZHWN4yF1sZmZmdc0JUokZk0fz6DNr6emJvEMxMzOznDhBKrFbx2g2bOph8aoNeYdiZmZmOXGCVGLGZD/JZmZmVu+cIJXYvSN7F5KfZDMzM6tfTpBKdIxtZXRLo59kMzMzq2NOkEpIYkbHGB582l1sZmZm9coJUhkH7jaJvz60jHufXJV3KGZmZpaDqiVIknaRdIOkBZLmS/pQ2n6WpCckzU2f11YrhsE6/fAXMGFUC5/7zTw/7m9mZlaHqtmC1AV8NCL2BA4C3i9pr1R2TkTMSp+rqxjDoEwY1cKZR72IOY8t5/I7n8g7HDMzM6uxqiVIEbE4Iv6ellcDC4Cp1Trftvbml+zM/rtO4CtXL2Dl+k15h2NmZmY1VJMxSJKmA/sBt6ZNp0u6W9IFkib2ss+pkuZImrN06dJahLmFhgbxhWP3Yfm6jXzj2vtrfn4zMzPLT9UTJEljgF8BZ0TEKuC7wO7ALGAx8I1y+0XEeRExOyJmd3R0VDvMsvaZOp53HzSNn93yGPOeWJlLDGZmZlZ7VU2QJDWTJUcXRcTlABGxJCK6I6IH+AFwQDVj2FofOWIPJo1u4f/92gO2zczM6kU1n2ITcD6wICK+WbR9p6JqbwDmVSuGbWF8ezOfOnpP5i5cwQ9vfpgIJ0lmZmYjXTVbkA4G3g0cXvJI/39KukfS3cBhwIerGMM28cb9p/LPMyfzH1ffx9u+fwv3LHJ3m5mZ2Uim4dAiMnv27JgzZ06uMXR193DJnIV889oHeGbtRt64/1Q+ceSL2HF8W65xmZmZWeUk3RERs/ur5zdpV6ipsYF3HjiNGz5+KP92yO5cdddiDvv6jVwz76m8QzMzM7NtzAnSAI1ra+bMo1/EHz96CDtNaOO7Nz2Ud0hmZma2jTlBGqRdJo3iTfvvzF0LV7Bk1Ya8wzEzM7NtyAnSVnjNXjsAcN29S3KOxMzMzLYlJ0hbYeb2Y5i+3SgnSGZmZiOME6StIIkj9t6Rvz60jNUbPF+bmZnZSOEEaSu9Zq8d2NQd3Hh/7eeLMzMzs+pwgrSV9t91ItuNbnE3m5mZ2QjiBGkrNTaIV++5Azfc9zQbu3ryDsfMzMy2ASdI28Br9tqB1Z1d3PLwM3mHYmZmZtuAE6Rt4BUzJ9Pe3OhuNjMzsxHCCdI20NbcyCEv7OC6e5cwHOa2MzMzs745QdpGXrPXDjy1agP3PLEy71DMzMxsKzlB2kYOf9H2NDaIa+e7m83MzGy46zdBkrSDpPMl/T6t7yXpvdUPbXiZOLqFA6ZP4tp7n8o7FDMzM9tKlbQg/Rj4AzAlrT8AnFGtgIaz1+y1Aw8sWcOjy9bmHYqZmZlthUoSpMkRcSnQAxARXUB3VaMapgqT115+5xM5R2JmZmZbo5IEaa2k7YAAkHQQ4JHIZewyaRSvffGOfP+mh3j8mXV5h2NmZmaDVEmC9BHgSmB3SX8BfgJ8oKpRDWOfe93eNDWIz/5mnh/5NzMzG6b6TZAi4u/AIcDLgdOAvSPi7moHNlztOL6Njx6xBzc9sJSr7/GAbTMzs+GokqfY3g+MiYj5ETEPGCPpfdUPbfg64WXT2GfqOM7+7XxWb9iUdzhmZmY2QJV0sZ0SESsKKxGxHDileiENf02NDXz5uBezdE0n37j2gbzDMTMzswGqJEFqkKTCiqRGoKV6IY0M++4ygXcfNI2f/O1R7lnkMe1mZmbDSSUJ0h+ASyW9StLhwM+Ba6ob1sjwsSP3YLsxrXz6invo7vGAbTMzs+GikgTp/7d35/Fx1eUexz/PTDLZuiRpuu+FUtqyFNpCK7IvF5BFlFVUtivqdWHRq+CGiN7rdUERFGRHUECgKKAoCCiLLC1Q2tLSFrpAF9J0S9skTTIzz/3jnLQhZJkuk5Nkvu/X67zm7OfJTCZ9+lu/CTwNfBH4EvAU8I2OLjKz4Wb2jJktMLM3zeyScH+5mT1pZovD17Jd+QG6sj6F+XzvpAnMXVnNH155N+pwREREJEOZ9GJLu/uN7n66u3/S3X/r7pkMFJkEvubu44FpwJfMbAJwBfCUu48lSLau2JUfoKs7ab/BTB/Tj2ufWEh1rRpsi4iIdAeZ9GI7JCzpWWRmS8xsqZkt6eg6d18dDhGAu28GFgBDgVOBu8LT7gI+vvPhd31mxndPmsDGukZ+9fTiqMMRERGRDGRSxXYbcC3wUWAqMCV8zZiZjQIOAF4GBrr7agiSKGBAG9dcbGazzGxWVVXVjjyuy5kwpA9nTx3OXf9expKqLVGHIyIiIh3IJEGqdvfH3X2Nu69rWjJ9gJn1Ah4CLnX3TZle5+43u/sUd5/Sv3//TC/rsi4/dhyF+XF+9JcFUYciIiIiHcgkQXrGzH5qZtPN7MCmJZObm1k+QXL0e3efEe6uNLPB4fHBwJqdiryb6d+7gC8ftSdPvbWGZxd17xIxERGRni6TBOlggmq1/wF+Hi4/6+iicOyk24AF7n5ts0OPAOeF6+cBf96RgLuzCw4ZxYjyYn74l/kkU+mowxEREZE25HV0grsfuZP3PgT4DDDXzGaH+74F/JhgXKWLgHeBM3by/t1OQV6cb504ni/c8yr3vvIun5k+KuqQREREpBUdJkgAZvYxYCJQ2LTP3X/Q3jXu/jxgbRw+OtMAe5r/mDiQaWPKufbJRZy8/xBKizUouYiISFeTSTf/m4CzgK8QJDxnACOzHFePZWZ876SJbNqa5OpH50cdjoiIiLQikzZIH3H3zwIb3P1qYDowPLth9WwThvThy0fuycOvr+TxuaujDkdERERayCRBqgtfa81sCNAIjM5eSLnhy0ftyb5D+/Kth+dStbk+6nBERESkmUwSpMfMrBT4KfAasAy4L5tB5YL8eIxrz9yfmoYUV86Yi7smsxUREekqMpmL7Rp33+juDxG0Pdrb3b+b/dB6vrEDe/ON/xjHPxZU8uCrK6IOR0REREJt9mIzs6Pc/Wkz+0Qrx2g28KPsggsPGc0T8yu5+tH5TN+jH8PKiqMOSUREJOe1V4J0ePh6civLSVmOK2fEYsbPz9gfd+e/H5hDOq2qNhERkai1WYLk7leZWQx43N3/2Ikx5Zzh5cV856QJXDljLg++toIzp6iToIiISJTabYPk7mngy50US047a8pwJo8s4/8ef4vq2saowxEREclpmfRie9LMvm5mw82svGnJemQ5JhYzfnDqRDbUNnDtkwujDkdERCSnZTLVyIXh65ea7XNgzO4PJ7dNHNKXT08byd0vLefMqcOZOKRv1CGJiIjkpEy6+Y9uZVFylCVfO3YcZcUJrvrzmxobSUREJCKZVLFhZvuY2Zlm9tmmJduB5aq+xfl88/i9mbV8Aw+/vjLqcERERHJSJpPVXgVcHy5HAj8BTslyXDnt9MnDmDS8lP/568ddk9kAACAASURBVFts2qoG2yIiIp0tkxKk04Gjgffd/QJgf6Agq1HluKYG2+tq6vnlk4ujDkdERCTnZDRZbdjdP2lmfYA1qIF21u03rJRzDhrBXS8uY3Hl5qjDERERySmZJEizwslqbwFeJZiw9pWsRiUAfP24cZQk4vzwLwuiDkVERCSnZNKL7b/CyWpvAo4Fzgur2iTLyksSXHLMXvxrURXPLFwTdTgiIiI5o80Eyczmm9m3zWyPpn3uvszd53ROaALwmWkjGVNRwg8fm09jKh11OCIiIjmhvRKkc4BewBNm9rKZXWpmQzopLgkl8mJ8+2PjeaeqhnteWh51OCIiIjmhzQTJ3d9w9yvdfQ/gEmAk8JKZPW1mn+u0CIWj9h7AoWMr+OU/FrOhpiHqcERERHq8jAaKdPeX3P0y4LNAGXBDVqOSDzAzvvOxCWze2sh1T6nbv4iISLZlMlDkVDO71syWA1cDNwNDsx6ZfMC4Qb351MEjuPul5by9Rt3+RUREsqm9Rtr/Y2bvADcCq4BD3P1wd7/R3dd2WoSyzWXH7EVxIs53/jSPmvpk1OGIiIj0WO2VINUDJ7j7FHf/mbuv6KygpHX9ehXwnY+N5+Wl6znhuueYuWx91CGJiIj0SO010r7a3Rft7I3N7HYzW2Nm85rt+76ZrTSz2eFy4s7eP1edNXUEf/z8dBznzN++yP/+dQH1yVTUYYmIiPQoGTXS3kl3Ase3sv8X7j4pXP6axef3WFNHlfP4JYdx9tQR/PbZJZxy/QvMW1kddVgiIiI9RtYSJHd/FlAdUJb0Ksjjfz+xL3ecP5X1tQ2cdP3zfOkPr6kBt4iIyG5g7t76AbMD27vQ3V/r8OZmo4DH3H2fcPv7wPnAJmAW8DV339DGtRcDFwOMGDFi8vLlGiSxLdW1jdzy3BLueGEpdY0pTp00lK8ePZbRFSVRhyYiItKlmNmr7j6lw/PaSZCeCVcLgSnAG4AB+wEvu/tHMwhiFB9MkAYCawEHrgEGu/uFHd1nypQpPmvWrI5Oy3nraxr47b/e4a4Xl9GYcr505J5cfuxeUYclIiLSZWSaILXXSPtIdz8SWA4cGPZmmwwcALy9M0G5e6W7p9w9DdwCHLQz95HWlZckuPLE8Tz7jSM5eb/B/Oqpxcx4TZ0PRUREdlQmbZD2dve5TRvuPg+YtDMPM7PBzTZPA+a1da7svAG9C/nZGfszbUw5V86YqwbcIiIiOyiTBGmBmd1qZkeY2eFmdguwoKOLzOxe4EVgnJmtMLOLgJ+Y2VwzmwMcCVy2S9FLm/LiMW741IGUlyT4wj2vag43ERGRHdBmG6RtJ5gVAl8EDgt3PQvc6O5bsxzbNmqDtPNmv7eRM296kYPHlHPnBQcRj1nUIYmIiERml9sgNQkToZuAK9z9NHf/RWcmR7JrJg0v5ZqPT+S5xWv56d8XRh2OiIhIt5DJZLWnALOBv4Xbk8zskWwHJrvPWVNH8KmDR3DTv97h8bmrow5HRESky8ukDdJVBL3NNgK4+2xgVBZjkiy46uQJ7D+8lG8+NIdVG+uiDkdERKRLyyRBSrq7ukF1cwV5ca47axLJtPO1P75BOt1+2zMREZFclkmCNM/MPgXEzWysmV0P/DvLcUkWjKoo4aqTJ/DiknXc/sLSqMMRERHpsjJJkL4CTATqgT8A1cCl2QxKsufMKcM5bsJAfvK3hSxYvSnqcERERLqkdhMkM4sDV7v7t919arh8R73Yui8z48ef3I++xflcet9stjamog5JRESky2k3QXL3FDC5k2KRTlJekuCnp+/HwsrN6vovIiLSirwMznk97Nb/AFDTtNPdZ2QtKsm6I8YN4LzpI7nt+aUcMa4/h47tH3VIIiIiXUYmbZDKgXXAUcDJ4XJSNoOSznHFCePZc0AvLrv/DdZsVq2piIhIkw6nGukKNNVI9ix8fzOn/vp5Jo8s43cXHqypSEREpEfLdKqRDqvYzOwO4ENZlLtfuJOxSRcyblBvrj5lIt98aC6/eeZtvnL02KhDEhERiVwmbZAea7ZeCJwGrMpOOBKFM6cM58V31vGLfyxi6uhypo3pF3VIIiIikcpkstqHmi2/B84E9sl+aNJZzIwfnrYvo/qVcMl9r7NuS33UIYmIiEQqk0baLY0FRuzuQCRavQryuOFTB7KhtpHLNRWJiIjkuA4TJDPbbGabmhbgUeCb2Q9NOtuEIX343kkT+NeiKj5160vMXLY+6pBEREQi0WEbJHfv3RmBSNdw7sEjSLvzq6cWc8ZNL3Lo2Aq+dtw4Jg0vjTo0ERGRTpNJCdJTmeyTnsHM+Oz0UTz7jSO58oS9mbeymo//+gUuvHMmj89dTU19MuoQRUREsq7NEiQzKwSKgQozKwOaBsjpAwzphNgkQsWJPD5/+B6cO20kd/17Gbc+t4Sn31pDIi/GR/es4NgJAzlm/ED69y6IOlQREZHdrs2BIs3sEuBSgmSoebf+TcAt7n5D9sMLaKDI6CVTaWYu28AT89/nyfmVrNhQR37c+M25kzl2wsCowxMREclIpgNFdjiStpl9xd2v322R7QQlSF2Lu/PW+5u5YsZcFqzexF0XHMT0PTR2koiIdH2ZJkiZdPO/1cwuN7MZZvaQmV0aVr9JjjIzxg/uw53nT2VkeTGf+90s5q6ojjosERGR3SaTBOkuYCJwPXADMAG4O5tBSfdQVpLg7osOprQ4n/PueIW312yJOiQREZHdIpMEaZy7X+Tuz4TLxcBe2Q5MuodBfQu556KDiZnx2dteZuXGuqhDEhER2WWZJEivm9m0pg0zOxh4oaOLzOx2M1tjZvOa7Ss3syfNbHH4WrZzYUtXMqqihN9deBCb65N8+taXVZIkIiLdXiYJ0sHAv81smZktA14EDjezuWY2p53r7gSOb7HvCuApdx8LPBVuSw8wYUgf7rxgKtV1jZx8/fM8MOs9OuoAICIi0lVl0ottZHvH3X15O9eOAh5z933C7YXAEe6+2swGA/9093EdBalebN1H5aatXHrfbF5cso7TDhjKNR/fh14FHQ7YLiIi0il2Wy82d18eJkF1gDctzfbviIHuvjq872pgQFsnmtnFZjbLzGZVVVXt4GMkKgP7FHLPfx7M5cfuxZ9nr+Tk659n3kr1cBMRke4lk6lGTjGzxcBS4F/AMuDxLMeFu9/s7lPcfUr//v2z/TjZjeIx46tHj+Xez02jriHFGTe9qGEARESkW8mkDdI1wDRgkbuPBo4mg0babagMq9YIX9fs5H2kGzh4TD8e+cohlJckuOiumayuVg83ERHpHjJJkBrdfR0QM7OYuz8DTNrJ5z0CnBeunwf8eSfvI93EgN6F3H7+VGobUlx45yy2aLJbERHpBjJJkDaaWS/gWeD3ZnYd0OG/cmZ2L0GPt3FmtsLMLgJ+DBwbVtkdG25LDzduUG9+fe6BLKrczFfvfZ1UWr3bRESka8ukF1sJQQPtGHAu0Bf4fViq1CnUi61nuPul5Xz3T/M4/yOj+P4pE6MOR0REclCmvdja7H9tZnsS9Dpram+UBu4ys8OAUqDTEiTpGT4zbSTL1tZw2/NLGV1RwnkfGRV1SCIiIq1qr4rtl8DmVvbXhsdEdti3ThzPMeMHcM1j85mzYmPU4YiIiLSqvQRplLt/aKRsd58FjMpaRNKjxWPGz8+YRP/eBVxy32xq1GhbRES6oPYSpMJ2jhXt7kAkd/QtzucXZ01i2boafvDo/KjDERER+ZD2EqSZZva5ljvD3mivZi8kyQXTxvTji4fvwf2z3uPxuaujDkdEROQD2psk61LgYTM7l+0J0RQgAZyW7cCk57vs2L144e21XDFjLvsPL2VIqQomRUSka2izBMndK939I8DVBNOLLAOudvfp7v5+54QnPVl+PMZ1Zx9AYyrN5X+crfGRRESky+hwmvVw5OxnOiEWyUGjKkr4/ikT+caDc/j6A29w1N4DmDikD6P6lRCLWdThiYhIjuowQRLJtjMmD2POio3c98p7PPz6SgCKE3H2HtSbcw4awRlThkccoYiI5JoOR9LuCjSSdm6oT6ZYXLmF+as3MX/VJl5eup4Fqzfx+cPH8M3/2FslSiIisst2eSRtkc5WkBdnn6F92WdoXwCSqTTff/RNfvuvJaxYX8fPz9yfwvx4xFGKiEguUIIkXVZePMY1p+7DyPISfvTXBby/aSu3fHYK5SWJqEMTEZEerr1xkEQiZ2Z87rAx/ObcA5m3sppP/OYFlq6tiTosERHp4ZQgSbdw4r6D+cPnprFpa5LTb/w3c1dURx2SiIj0YEqQpNuYPLKMB78wncL8OGff/CIvvL026pBERKSHUoIk3cqY/r2Y8V8fYVhZMRfcMZO/zNE0JSIisvspQZJuZ2CfQv74+ensN6wvX773Ne5+aXnUIYmISA+jBEm6pb7F+dx90cEcNW4A3/3TPL54z6vMW6l2SSIisnsoQZJuqygR56bPTOarR+3J84vXctL1z3Pe7a8wc9n6qEMTEZFuTiNpS49QXdfIPS8t57bnl7K+poHJI8vYd2hfKnol6NergH4lCQb1LWSfIX01IreISA7LdCRtJUjSo9Q1pLhv5rvc+8q7rN64lc31yQ8cH15exNlTR3DG5GEM6FMYUZQiIhIVJUgiwNbGFOtrGli3pYFFlZt54NX3eGnJeuIx46i9B3DOQcM5bGx/8uKqbRYRyQVKkETasKRqC/fPfI8HX13BupoGKnolOGm/IZx2wFD2G9YXM1XBiYj0VEqQRDrQkEzz9Ftr+PPslTy1YA0NqTRjKkr45ORhfOqgEZRpzjcRkR5HCZLIDqiua+Txuat5+PWVvLx0PUX5cc6cMoz/PHQMw8uLow5PRER2ky6dIJnZMmAzkAKSHQWqBEk606LKzdz87BL+PHslqbRz4r6D+a8j9mTCkD5RhyYiIruoOyRIU9w9o8m0lCBJFN6v3sod/17KH156l63JFD85fT9OO2BY1GGJiMguyDRBUtcdkTYM6lvIlSeM57lvHsnkkWVcdv8b/OqpxXSHamkREdk1USVIDjxhZq+a2cWtnWBmF5vZLDObVVVV1cnhiWxXWpzgdxcezCcOGMq1Ty7ivx+cQ0MyHXVYIiKSRXkRPfcQd19lZgOAJ83sLXd/tvkJ7n4zcDMEVWxRBCnSJJEX4+dn7s+IfsX88h+LWbWxjhs/PZm+RflRhyYiIlkQSYLk7qvC1zVm9jBwEPBs+1eJRMvMuPSYvRhWVsyVM+Zw6P89zRHjBnD0+AEcvld/Sos1LICISE/R6QmSmZUAMXffHK4fB/ygs+MQ2VmnTx7GmP4l3PvyuzyzcA2PvLGKeMyYPLKMTxwwlE9OHka+RuYWEenWOr0Xm5mNAR4ON/OAP7j7j9q7Rr3YpKtKp503Vmzk6bfW8MSblSys3MyI8mK+evRYPj5piKYwERHpYrp0N/8dpQRJugN355mFa/j5E4t4c9UmxvQv4dJj9uJj+w4mHtP0JSIiXYESJJGIuDt/f/N9rn1yEYsqt1CQF2PcoN6MH9SHvQf3ZvzgPuw/rJSiRDzqUEVEco4SJJGIpdLOk/MrmblsPW+9v4kFqzezvqYBgEQ8xuSRZXx0bAUf3bOCfYb2VSmTiEgnUIIk0sW4O1Wb63lz1SZeXLKO5xavZcHqTQCUFudzwj6DOWPKMA4YXoqZkiURkWzINEGKahwkkZxjZgzoU8iAPoUcufcAANZuqeeFt9fyz4VV/On1ldz7yrvs0b+E0ycP5xMHDmVgn8KIoxYRyU0qQRLpIrbUJ/nLnFU8+OoKZi7bQMxg2ph+nLz/EI6fOIiyEo2zJCKyq1TFJtKNLV1bw4zXVvDoG6tYtq6WvJhx6NgKTtx3MJNHljGqXwkxtVkSEdlhSpBEegB3581Vm3j0jVU8+sYqVlVvBaB3QR4Th/Zh36F9OWBE0Ni7T6GmPRER6YgSJJEeJp12FlZuZu6KauaurGbOymoWrN5EQzJNftyYNqYfx4wfyDETBjK0tCjqcEVEuiQlSCI5oDGVZvZ7G/nH/EqeXFDJkqoaAEZXlDCsrIghfYsYUlrE4NJCBvctpKJXAf17F1BWnNCwAiKSk5QgieSgd6q28I/5lcx+byOrqreyamMdVZvrP3RezKBfrwLGD+7DcRMGctyEgQxQjzkRyQFKkEQEgPpkivert1K5qZ61W+qp2hy8rtlUzyvL1rN0bQ1mcMDwUo6bOIjjJw5iVEVJ1GGLiGSFEiQR6ZC7s3jNFv4+733+Pv995q0MBq7ce1DvbcnS+MG9NXCliPQYSpBEZIet2FDLE29W8rc332fmsvW4w/DyIsZU9KKkIE5xIo+SRJySgjwG9y1keHkxI8qLGVpWREGe5pYTka5PCZKI7JKqzfX8Y0ElTy1YQ9WWemrrk9Q2pKhpSLJla5JkevvfDjMY2LuQfr0SlBUnKCtJUFacT0WvAsYN6s2EwX0YVlakkigRiZymGhGRXdK/dwHnHDSCcw4a8aFj6bSzdks9766v3bas2FDHhpoG1tc2sHJjHetrGqiua9x2Te/CPMYP7sPYAb3oV5KgtDhBaXE+ZcUJKnoVMLKiWGM5iUiXoQRJRHZYLLZ9Xrkpo8rbPK+uIcXCys3MX7WJ+aurmb9qE3+Zu5rqukZaK7wuL0kwql8xoypKGN2vhDH9ezG6ooTRFSUUJVSFJyKdRwmSiGRNUSLOpOGlTBpe+oH9qbSzeWsjG2ob2VjbQOWmepavq2HZuhqWra3lxXfWMeO1lR+4ZmhpEaMqihlWWszw8iKGlxczrKyIAb0LKS3Op1dBnqrwRGS3UYIkIp0uHrOwii0BtD6kQG1DkqVra1hSFS5rt7B8XS1PvVXJ2i0Nrd+zKJ++xfn0K0nQr6SA8l4JKkoS9OtVwIh+xezZvxdDSos0SKaIdEgJkoh0ScWJPCYO6cvEIX0/dKy2IcnKDXW8t6GWtVsaqK5tZGNdw7YSqfU1DSxZu4WZy4I2Uc2r8wryYtuq7foU5lOUiAdLfpzC/Bh5sRh5cQteY0Z+nlGUH6cokUdxeF7vwjzKShL0VqmVSI+lBElEup3iRB5jB/Zm7MDeHZ6bSjvraxpYvq6Gd6q28E5VDe+s2cLCys3U1Cepa0ixtTFNQyq9w3HkhSVh5SX59C3KpyiRR1F+jOJEHkWJOCWJcGiEgu2vvQvyKS0Ozu8bvmqIBJGuRwmSiPRo8ZjRv3cwB117DcqTqTRbk2mSqTTJtJNMOcl0moZkmrrGFHUNKWrDZUt9cltJ1YbaBjbUNFJd18imukYqq1PUNiY/cH5HzCA/FiMes7D0ykjkxSjIi4evwVJWnKBfr6DKsF9JMKRCQVjqlcgLSr3y4zEK8puuiW+7NhYzYmbEzYjFCM814jFTKZhIK5QgiYgAefEYveKx3X7fdNqpawzGj6qpT7F5ayMba4OEamNdI9W1DdQn0zSmnFR6e3LWkExTn0zRkEpT35hmazLF6uqtzFtVzbotDR8Yh2pXJeJBtWJxIk5pcTCGVd+i4LWkII+YBYlbPBYmb/EYhWGVZPAaJz8eVEnmxW3ben5ejEQ8RiIvSNzyw2PxmJHfVJUZD9ZjahcmXYwSJBGRLIrFjJKCPEoK8qDjGsGMuDubtibZUNNAQypNYypIsJKpoMSrPkyq6pMp6pPBvrQ76bST8iBpCxKx8Nq005hMU9OQYmNtUCq2YkMtc1c2sLUxTSodlKal09CYTrc6RMOuihlhErU9kcqPB6Vf+WECFzMjZmDha14sKC1rSsK2lZqFJWiF+UEJWl48RswIStDCkrRYLChNi8e278+Lx0g0JXhhHAV5MRLxoCSvacmLbU8WY2Gylx8PrpeeQwmSiEg3Y2ZBG6aiaAbWbEyl2dqYoq4xFZRuNQYlXam0b0vUkmmnMUzYGlPN1tOtn5dMOY3pNI3JcDud3lay1pgMEjmHINHzIElsTKWpqU+yPkwCm5LBrcnUtlK3zpwsImYESVSYsFlTlaYFiXI81qx0LR7bVp1qGGZBVWtTspYIE8Sm0rfmBWxNNaLxZtWk20vwYuSHpXdNz2lKBpviiJlhze5jxrYYtiWQYRzxWPCcvHB/U8zbSgLDxDW4z/b7xqzpZ7JtyWnT/qbnx8Lq3qZnNiWeXaXKN5IEycyOB64D4sCt7v7jKOIQEZEd11S607uLj3weJFFh6ZcHDfaDUjQPS9QI1tO+rZSsKZkLEjqnIUzsGpJpGlIpGpJBUtdUCteU7DUlgA3NXr35Mzx8Riq4ZzLVVK3qOI47pNOQ9DQp9zCxDM5pSAb3AvBtPxtBKWA6TSpMLoN2c11/+rCOPPCF6Uxtp71gZ+n0BMnM4sCvgWOBFcBMM3vE3ed3diwiItJzmRmJPCNB7lR9ufu2UrmmUrqUBwlYKkzo3MHDVKuphK15yVyqKbELk8mm65qWbclYWD3rvv2e7oTrwT2bjjXF4OFzmp6XbvaMpqRzSGlRdG9gM1GUIB0EvO3uSwDM7D7gVEAJkoiIyC4ws21tuGTXRPEODgXea7a9Itz3AWZ2sZnNMrNZVVVVnRaciIiISBQJUmutrz5UaeruN7v7FHef0r9//04IS0RERCQQRYK0AhjebHsYsCqCOERERERaFUWCNBMYa2ajzSwBnA08EkEcIiIiIq3q9Eba7p40sy8Dfyfo5n+7u7/Z2XGIiIiItCWScZDc/a/AX6N4toiIiEhH1A9QREREpAUlSCIiIiItWNPw5V2ZmVUBy7P4iApgbRbvL5nR59A16HPoGvQ5dA36HKK3uz+Dke7e4fhB3SJByjYzm+XuU6KOI9fpc+ga9Dl0DfocugZ9DtGL6jNQFZuIiIhIC0qQRERERFpQghS4OeoABNDn0FXoc+ga9Dl0DfocohfJZ6A2SCIiIiItqARJREREpAUlSCIiIiIt5HyCZGbHm9lCM3vbzK6IOp5cYGbDzewZM1tgZm+a2SXh/nIze9LMFoevZVHHmgvMLG5mr5vZY+H2aDN7Ofwc7g8nlZYsMrNSM3vQzN4KvxfT9X3ofGZ2Wfg3aZ6Z3Wtmhfo+ZJ+Z3W5ma8xsXrN9rf7+W+BX4b/Zc8zswGzFldMJkpnFgV8DJwATgHPMbEK0UeWEJPA1dx8PTAO+FL7vVwBPuftY4KlwW7LvEmBBs+3/A34Rfg4bgIsiiSq3XAf8zd33BvYn+Dz0fehEZjYU+Cowxd33IZhM/Wz0fegMdwLHt9jX1u//CcDYcLkYuDFbQeV0ggQcBLzt7kvcvQG4Dzg14ph6PHdf7e6vheubCf4xGErw3t8VnnYX8PFoIswdZjYM+Bhwa7htwFHAg+Ep+hyyzMz6AIcBtwG4e4O7b0TfhyjkAUVmlgcUA6vR9yHr3P1ZYH2L3W39/p8K/M4DLwGlZjY4G3HleoI0FHiv2faKcJ90EjMbBRwAvAwMdPfVECRRwIDoIssZvwS+AaTD7X7ARndPhtv6TmTfGKAKuCOs6rzVzErQ96FTuftK4GfAuwSJUTXwKvo+RKWt3/9O+3c71xMka2Wfxj3oJGbWC3gIuNTdN0UdT64xs5OANe7+avPdrZyq70R25QEHAje6+wFADapO63RhG5dTgdHAEKCEoDqnJX0fotVpf6NyPUFaAQxvtj0MWBVRLDnFzPIJkqPfu/uMcHdlU1Fp+LomqvhyxCHAKWa2jKB6+SiCEqXSsIoB9J3oDCuAFe7+crj9IEHCpO9D5zoGWOruVe7eCMwAPoK+D1Fp6/e/0/7dzvUEaSYwNuylkCBokPdIxDH1eGE7l9uABe5+bbNDjwDnhevnAX/u7Nhyibtf6e7D3H0Uwe/+0+5+LvAMcHp4mj6HLHP394H3zGxcuOtoYD76PnS2d4FpZlYc/o1q+hz0fYhGW7//jwCfDXuzTQOqm6ridrecH0nbzE4k+F9zHLjd3X8UcUg9npl9FHgOmMv2ti/fImiH9EdgBMEfqzPcvWXDPckCMzsC+Lq7n2RmYwhKlMqB14FPu3t9lPH1dGY2iaChfAJYAlxA8B9YfR86kZldDZxF0NP2deA/Cdq36PuQRWZ2L3AEUAFUAlcBf6KV3/8web2BoNdbLXCBu8/KSly5niCJiIiItJTrVWwiIiIiH6IESURERKQFJUgiIiIiLShBEhEREWlBCZKIiIhIC0qQRAQAM9uyg+cfYWaPZSue8Bn3hjN2X5bh+aeYWeSjUJvZnWZ2esdnikhXldfxKSIinc/MBgEfcfeRmV7j7o+gwV5FZDdQCZKIfEBYMvRPM3vQzN4ys9+Hg7NhZseH+54HPtHsmhIzu93MZoYTrp4a7r/czG4P1/c1s3lmVtzieYVmdoeZzQ2vPTI89AQwwMxmm9mhLa7pb2YPhc+baWaHhPvPN7MbwvU9zOyl8PgPmpeQmdl/h/vnhIMDYmajzGyBmd1iZm+a2RNmVmRm483slWbXjjKzOeH698L7zDOzm5vepxaxLjOzinB9ipn9s733TES6BiVIItKaA4BLgQkEs80fYmaFwC3AycChwKBm53+bYKqSqcCRwE/DGel/CexpZqcBdwCfd/faFs/6EoC77wucA9wVPusU4B13n+Tuz7W45jrgF+HzPkkwCnVL1wHXhedsm6vJzI4DxgIHAZOAyWZ2WHh4LPBrd58IbAQ+6e4LgEQ4wjgEIy3/MVy/wd2nuvs+QBFwUmtvZhvaes9EpAtQgiQirXnF3Ve4exqYDYwC9iaYzHOxB0Pw39Ps/OOAK8xsNvBPoBAYEV5/PnA38C93f6GVZ300PI67vwUsB/bqIL5jgBvC5z0C9DGz3i3OmQ48EK7/oUWsxxFMG/Fa+HONDY8tdffZ4fqr4c8NQUJ0Zrh+FnB/uH6kmb1sZnMJJvud2EHczbX6nu3A9SKSRWqDJCKtaT7XVIrtfyvampvICEpbFrZybCywBRjSzrU7KgZMd/e6D9zowzVcbT3vUsk7LgAAAYZJREFUf939ty2uHcWHf+6icP1+4AEzmwG4uy8OS7l+A0xx9/fM7PsESU5LSbb/Z7T58fbeMxGJmEqQRCRTbwGjzWyPcPucZsf+DnylWVulA8LXvgRVXYcB/dro2fUscG54/l4EpSgdJQ1PAF9u2ggne23pJYLqN4CzW8R6oZn1Cq8damYD2nuYu79DkDB9l+2lR03JztrwXm31WlsGTA7XP9lsf6vvmYh0DUqQRCQj7r4VuBj4S9hIe3mzw9cA+cAcM5sXbgP8AviNuy8CLgJ+3Eoy8hsgHlZT3Q+cn8Fs6V8FpoSNrOcDX2jlnEuBy8MG1oOB6vDneIKgyu3F8JkPAi2r51pzP/BpwvZH7r6RoE3WXIKZx2e2cd3VwHVm9hxBktWkrfdMRLoAC5oSiIj0LGFvuTp3dzM7GzjH3dVTTEQyojZIItJTTSZoyG0EPdIujDgeEelGVIIkIiIi0oLaIImIiIi0oARJREREpAUlSCIiIiItKEESERERaUEJkoiIiEgL/w9d8MXpHOkbXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of total data variance captured by 500 largest eigenvectors: 0.9292050565369442\n",
      "Percentage of total data variance captured by 500 largest eigenvectors: 0.9733843233063826\n",
      "Percentage of total data variance captured by 500 largest eigenvectors: 0.9896934896951645\n"
     ]
    }
   ],
   "source": [
    "pca_train =PCA()\n",
    "pca_train.fit(train_feat)\n",
    "pca_train_eigvals = pca_train.explained_variance_\n",
    "\n",
    "x = np.arange(0, pca_train_eigvals.shape[0])\n",
    "fig, axs = plt.subplots(2, 1, constrained_layout = True, figsize = (8, 8))\n",
    "axs[0].plot(x, pca_train_eigvals, '-')\n",
    "axs[0].set_title('All Eigenvalues')\n",
    "axs[0].set_xlabel(\"Index of eigenvalue\")\n",
    "axs[0].set_ylabel(\"Captured Variance\")\n",
    "\n",
    "fig.suptitle('Eigenvalues of the covariance matrix')\n",
    "axs[1].plot(x[:100], pca_train_eigvals[:100], '-')\n",
    "axs[1].set_title('100 Largest Eigenvalues')\n",
    "axs[1].set_xlabel(\"Index of eigenvalue\")\n",
    "axs[1].set_ylabel(\"Captured Variance\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of total data variance captured by 500 largest eigenvectors:\", \n",
    "      (np.sum(pca_train_eigvals[:100]))/(np.sum(pca_train_eigvals[:100])+np.sum(pca_train_eigvals[100:])))\n",
    "print(\"Percentage of total data variance captured by 500 largest eigenvectors:\", \n",
    "      (np.sum(pca_train_eigvals[:250]))/(np.sum(pca_train_eigvals[:250])+np.sum(pca_train_eigvals[250:])))\n",
    "print(\"Percentage of total data variance captured by 500 largest eigenvectors:\", \n",
    "      (np.sum(pca_train_eigvals[:500]))/(np.sum(pca_train_eigvals[:500])+np.sum(pca_train_eigvals[500:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate for reduced dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA with 100 dimensions:\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n",
      "\n",
      "PCA with 250 dimensions:\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n",
      "\n",
      "PCA with 500 dimensions:\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in [100, 250, 500]:\n",
    "    pca_train_noval =PCA(n_components = i)\n",
    "    pca_train_noval.fit(train_noval_feat)\n",
    "    print(\"\\nPCA with {} dimensions:\".format(i))\n",
    "    kNN_val(pca_train_noval.transform(val_query_feat), pca_train_noval.transform(val_gallery_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate PCA feature transform on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA with 100 dimensions:\n",
      "Mean average precision: 0.43453251906823337\n",
      "Top1: 0.4685714285714286\n",
      "Top5: 0.6764285714285714\n",
      "Top10: 0.7485714285714286\n",
      "Top5-Precision: 0.34271428571428575\n",
      "Top5-Recall: 0.45023809523809516\n",
      "\n",
      "PCA with 250 dimensions:\n",
      "Mean average precision: 0.43549765512265515\n",
      "Top1: 0.4685714285714286\n",
      "Top5: 0.6728571428571428\n",
      "Top10: 0.75\n",
      "Top5-Precision: 0.3431428571428572\n",
      "Top5-Recall: 0.4513690476190476\n",
      "\n",
      "PCA with 500 dimensions:\n",
      "Mean average precision: 0.4356578025149454\n",
      "Top1: 0.4692857142857143\n",
      "Top5: 0.6714285714285714\n",
      "Top10: 0.7492857142857143\n",
      "Top5-Precision: 0.34271428571428575\n",
      "Top5-Recall: 0.45077380952380947\n"
     ]
    }
   ],
   "source": [
    "for i in [100, 250, 500]:\n",
    "    pca_train = PCA(n_components=i)\n",
    "    pca_train.fit(train_feat)\n",
    "    print(\"\\nPCA with {} dimensions:\".format(i))\n",
    "    kNN(pca_train.transform(query_feat), pca_train.transform(gallery_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL PCA TRANSFORMATIONS - USED IN THE CODE LATER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 500)\n",
    "train_feat_PCA_500 = pca.fit_transform(train_feat)\n",
    "query_feat_PCA_500 = pca.transform(query_feat)\n",
    "gallery_feat_PCA_500 = pca.transform(gallery_feat)\n",
    "train_noval_feat_PCA_500 = pca.fit_transform(train_noval_feat)\n",
    "val_query_feat_PCA_500 = pca.transform(val_query_feat)\n",
    "val_gallery_feat_PCA_500 = pca.transform(val_gallery_feat)\n",
    "\n",
    "pca = PCA(n_components = 50)\n",
    "train_feat_PCA_50 = pca.fit_transform(train_feat)\n",
    "query_feat_PCA_50 = pca.transform(query_feat)\n",
    "gallery_feat_PCA_50 = pca.transform(gallery_feat)\n",
    "train_noval_feat_PCA_50 = pca.fit_transform(train_noval_feat)\n",
    "val_query_feat_PCA_50 = pca.transform(val_query_feat)\n",
    "val_gallery_feat_PCA_50 = pca.transform(val_gallery_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate for reduced dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA with 50 dimensions:\n",
      "Mean average precision: 0.9839674563292653\n",
      "Top1: 0.9949748743718593\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7437185929648241\n",
      "Top5-Recall: 0.9861809045226131\n",
      "\n",
      "LDA with 100 dimensions:\n",
      "Mean average precision: 0.9970077661032434\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7517587939698491\n",
      "Top5-Recall: 0.9962311557788944\n",
      "\n",
      "LDA with 250 dimensions:\n",
      "Mean average precision: 0.9985076899649763\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7527638190954772\n",
      "Top5-Recall: 0.9974874371859297\n",
      "\n",
      "LDA with 500 dimensions:\n",
      "Mean average precision: 0.99945180447693\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 100, 250, 500]:\n",
    "    lda_train_noval =LinearDiscriminantAnalysis(n_components = i)\n",
    "    lda_train_noval.fit(train_noval_feat, train_noval_labels)\n",
    "    print(\"\\nLDA with {} dimensions:\".format(i))\n",
    "    kNN_val(lda_train_noval.transform(val_query_feat), lda_train_noval.transform(val_gallery_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate LDA feature transform on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA with 50 dimensions:\n",
      "Mean average precision: 0.3436014996907854\n",
      "Top1: 0.39285714285714285\n",
      "Top5: 0.5771428571428572\n",
      "Top10: 0.665\n",
      "Top5-Precision: 0.274\n",
      "Top5-Recall: 0.35892857142857143\n",
      "\n",
      "LDA with 100 dimensions:\n",
      "Mean average precision: 0.36187585034013603\n",
      "Top1: 0.41785714285714287\n",
      "Top5: 0.6007142857142858\n",
      "Top10: 0.6935714285714286\n",
      "Top5-Precision: 0.28585714285714287\n",
      "Top5-Recall: 0.37464285714285717\n",
      "\n",
      "LDA with 250 dimensions:\n",
      "Mean average precision: 0.3757265254586683\n",
      "Top1: 0.42428571428571427\n",
      "Top5: 0.6221428571428571\n",
      "Top10: 0.7128571428571429\n",
      "Top5-Precision: 0.2962857142857143\n",
      "Top5-Recall: 0.3884523809523809\n",
      "\n",
      "LDA with 500 dimensions:\n",
      "Mean average precision: 0.3745382395382395\n",
      "Top1: 0.41714285714285715\n",
      "Top5: 0.6221428571428571\n",
      "Top10: 0.7142857142857143\n",
      "Top5-Precision: 0.2988571428571428\n",
      "Top5-Recall: 0.39220238095238097\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 100, 250, 500]:\n",
    "    lda_train = LinearDiscriminantAnalysis(n_components=i)\n",
    "    lda_train.fit(train_feat, train_labels)\n",
    "    print(\"\\nLDA with {} dimensions:\".format(i))\n",
    "    kNN(lda_train.transform(query_feat), lda_train.transform(gallery_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL LDA TRANSFORMATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components = 500)\n",
    "train_feat_LDA = lda.fit_transform(train_feat, train_labels)\n",
    "query_feat_LDA = lda.transform(query_feat)\n",
    "gallery_feat_LDA = lda.transform(gallery_feat)\n",
    "train_noval_feat_LDA = lda.fit_transform(train_noval_feat, train_noval_labels)\n",
    "val_query_feat_LDA = lda.transform(val_query_feat)\n",
    "val_gallery_feat_LDA = lda.transform(val_gallery_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAHALANOBIS DISTANCE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Original covariance based approach </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on all dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 0.979396984924623\n",
      "Top1: 0.9949748743718593\n",
      "Top5: 0.9949748743718593\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7386934673366833\n",
      "Top5-Recall: 0.9761306532663316\n"
     ]
    }
   ],
   "source": [
    "Cov_Model_val = Covariance().fit(train_noval_feat, train_noval_labels)\n",
    "val_query_feat_Cov = Cov_Model_val.transform(val_query_feat)\n",
    "val_gallery_feat_Cov = Cov_Model_val.transform(val_gallery_feat)\n",
    "kNN_val(val_query_feat_Cov, val_gallery_feat_Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on reduced dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA-COV with 50 dimensions:\n",
      "Mean average precision: 0.9992690726359068\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n",
      "\n",
      "LDA-COV with 50 dimensions:\n",
      "Mean average precision: 0.9747123061193412\n",
      "Top1: 0.9849246231155779\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7396984924623115\n",
      "Top5-Recall: 0.9811557788944724\n",
      "\n",
      "PCA-COV with 100 dimensions:\n",
      "Mean average precision: 0.999725902238465\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n",
      "\n",
      "LDA-COV with 100 dimensions:\n",
      "Mean average precision: 0.9948073701842546\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7497487437185929\n",
      "Top5-Recall: 0.9937185929648241\n",
      "\n",
      "PCA-COV with 250 dimensions:\n",
      "Mean average precision: 0.9995431703974418\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7537688442211053\n",
      "Top5-Recall: 0.9987437185929648\n",
      "\n",
      "LDA-COV with 250 dimensions:\n",
      "Mean average precision: 0.9946811981987861\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7497487437185929\n",
      "Top5-Recall: 0.9937185929648241\n",
      "\n",
      "PCA-COV with 500 dimensions:\n",
      "Mean average precision: 0.9987361047662555\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7537688442211053\n",
      "Top5-Recall: 0.9987437185929648\n",
      "\n",
      "LDA-COV with 500 dimensions:\n",
      "Mean average precision: 0.9949291914116035\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7497487437185929\n",
      "Top5-Recall: 0.9937185929648241\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 100, 250, 500]:\n",
    "    print(\"\\nPCA-COV with {} dimensions:\".format(i))\n",
    "    pca_train_noval = PCA(n_components=i)\n",
    "    train_noval_feat_PCA_t = pca_train_noval.fit_transform(train_noval_feat)\n",
    "    val_query_feat_PCA_t = pca_train_noval.transform(val_query_feat) \n",
    "    val_gallery_feat_PCA_t = pca_train_noval.transform(val_gallery_feat)\n",
    "    Cov_Model_val = Covariance().fit(train_noval_feat_PCA_t, train_noval_labels)\n",
    "    val_query_feat_Cov_PCA = Cov_Model_val.transform(val_query_feat_PCA_t)\n",
    "    val_gallery_feat_Cov_PCA = Cov_Model_val.transform(val_gallery_feat_PCA_t)\n",
    "    kNN_val(val_query_feat_Cov_PCA, val_gallery_feat_Cov_PCA)\n",
    "    \n",
    "    print(\"\\nLDA-COV with {} dimensions:\".format(i))\n",
    "    lda_train_noval = LinearDiscriminantAnalysis(n_components=i)\n",
    "    train_noval_feat_LDA_t = lda_train_noval.fit_transform(train_noval_feat, train_noval_labels)\n",
    "    val_query_feat_LDA_t = lda_train_noval.transform(val_query_feat) \n",
    "    val_gallery_feat_LDA_t = lda_train_noval.transform(val_gallery_feat)\n",
    "    Cov_Model_val = Covariance().fit(train_noval_feat_LDA_t, train_noval_labels)\n",
    "    val_query_feat_Cov_LDA = Cov_Model_val.transform(val_query_feat_LDA_t)\n",
    "    val_gallery_feat_Cov_LDA = Cov_Model_val.transform(val_gallery_feat_LDA_t)\n",
    "    kNN_val(val_query_feat_Cov_LDA, val_gallery_feat_Cov_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on all dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 0.2337029220779221\n",
      "Top1: 0.3092857142857143\n",
      "Top5: 0.455\n",
      "Top10: 0.5014285714285714\n",
      "Top5-Precision: 0.18885714285714283\n",
      "Top5-Recall: 0.2478571428571428\n"
     ]
    }
   ],
   "source": [
    "Cov_Model = Covariance().fit(train_feat, train_labels)\n",
    "query_feat_Cov = Cov_Model.transform(query_feat)\n",
    "gallery_feat_Cov = Cov_Model.transform(gallery_feat)\n",
    "kNN(query_feat_Cov, gallery_feat_Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on reduced dimensions (PCA-50-dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 0.390218537414966\n",
      "Top1: 0.4307142857142857\n",
      "Top5: 0.6371428571428571\n",
      "Top10: 0.7107142857142857\n",
      "Top5-Precision: 0.307\n",
      "Top5-Recall: 0.4027380952380952\n"
     ]
    }
   ],
   "source": [
    "Cov_Model = Covariance().fit(train_feat_PCA_50, train_labels)\n",
    "query_feat_Cov_PCA = Cov_Model.transform(query_feat_PCA_50)\n",
    "gallery_feat_Cov_PCA = Cov_Model.transform(gallery_feat_PCA_50)\n",
    "kNN(query_feat_Cov_PCA, gallery_feat_Cov_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Large Margin Nearest Neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on all dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LargeMarg_Model = LMNN(verbose=True).fit(train_noval_feat,train_noval_labels)\n",
    "val_query_feat_LMNN = LargeMarg_Model.transform(val_query_feat)\n",
    "val_gallery_feat_LMNN = LargeMarg_Model.transform(val_gallery_feat)\n",
    "kNN_val(val_query_feat_LMNN,val_gallery_feat_LMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on reduced dimensions - PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 426205.3505330503 -1986.03270395234 46 1.0099999999999999e-07\n",
      "3 424209.49658753513 -1995.8539455151767 46 1.0201e-07\n",
      "4 422199.46581847203 -2010.0307690631016 46 1.030301e-07\n",
      "5 420175.5792956003 -2023.8865228717332 45 1.0406040099999999e-07\n",
      "6 418139.40073424025 -2036.1785613600514 44 1.0510100500999999e-07\n",
      "7 416089.97406359215 -2049.4266706480994 44 1.0615201506009999e-07\n",
      "8 414026.41549335717 -2063.558570234978 43 1.0721353521070098e-07\n",
      "9 411950.9807652671 -2075.4347280900693 41 1.08285670562808e-07\n",
      "10 409864.33312955755 -2086.6476357095526 39 1.0936852726843608e-07\n",
      "11 407764.9992526966 -2099.3338768609683 38 1.1046221254112044e-07\n",
      "12 405652.3872425703 -2112.612010126293 35 1.1156683466653164e-07\n",
      "13 403528.99305547157 -2123.3941870987182 35 1.1268250301319696e-07\n",
      "14 401391.2683750809 -2137.7246803906746 34 1.1380932804332893e-07\n",
      "15 399240.5115192417 -2150.756855839165 34 1.1494742132376222e-07\n",
      "16 397075.40797966864 -2165.103539573087 34 1.1609689553699984e-07\n",
      "17 394895.9582233088 -2179.449756359856 34 1.1725786449236985e-07\n",
      "18 392702.1656148487 -2193.792608460062 34 1.1843044313729355e-07\n",
      "19 390494.6752672554 -2207.4903475933243 33 1.1961474756866649e-07\n",
      "20 388274.09567424597 -2220.579593009432 31 1.2081089504435316e-07\n",
      "21 386041.41978231503 -2232.675891930936 30 1.220190039947967e-07\n",
      "22 383796.0590217676 -2245.3607605474535 29 1.2323919403474465e-07\n",
      "23 381536.913034405 -2259.145987362601 28 1.244715859750921e-07\n",
      "24 379264.583967846 -2272.329066558974 27 1.2571630183484302e-07\n",
      "25 376979.1184908802 -2285.4654769658227 27 1.2697346485319146e-07\n",
      "26 374679.59935199644 -2299.519138883741 26 1.2824319950172337e-07\n",
      "27 372366.9077758147 -2312.6915761817363 26 1.295256314967406e-07\n",
      "28 370040.0631426525 -2326.8446331621963 26 1.30820887811708e-07\n",
      "29 367699.3453371789 -2340.7178054735996 25 1.321290966898251e-07\n",
      "30 365345.3391646182 -2354.0061725606793 25 1.3345038765672335e-07\n",
      "31 362977.621581228 -2367.717583390244 24 1.347848915332906e-07\n",
      "32 360596.93786643655 -2380.683714791434 24 1.361327404486235e-07\n",
      "33 358203.8520845282 -2393.0857819083612 20 1.3749406785310975e-07\n",
      "34 355800.0756027894 -2403.7764817387797 20 1.3886900853164084e-07\n",
      "35 353382.89527429745 -2417.180328491959 19 1.4025769861695725e-07\n",
      "36 350952.4390215634 -2430.4562527340604 19 1.4166027560312682e-07\n",
      "37 348508.306005888 -2444.133015675412 19 1.430768783591581e-07\n",
      "38 346050.57307841367 -2457.7329274743097 19 1.4450764714274967e-07\n",
      "39 343579.32215253136 -2471.250925882312 19 1.4595272361417717e-07\n",
      "40 341094.64034080756 -2484.6818117238 19 1.4741225085031894e-07\n",
      "41 338596.7585817326 -2497.8817590749823 18 1.4888637335882214e-07\n",
      "42 336086.6324241602 -2510.1261575723765 17 1.5037523709241035e-07\n",
      "43 333564.6243715825 -2522.008052577672 16 1.5187898946333445e-07\n",
      "44 331030.33795827103 -2534.2864133114927 16 1.533977793579678e-07\n",
      "45 328483.1236171707 -2547.2143411003053 16 1.5493175715154747e-07\n",
      "46 325923.2850968088 -2559.8385203619255 15 1.5648107472306294e-07\n",
      "47 323351.2672700155 -2572.017826793308 15 1.5804588547029357e-07\n",
      "48 320766.912602143 -2584.3546678724815 14 1.5962634432499652e-07\n",
      "49 318170.75150503137 -2596.161097111646 14 1.6122260776824649e-07\n",
      "50 315562.7613810331 -2607.9901239982573 13 1.6283483384592896e-07\n",
      "51 312943.0619557369 -2619.6994252962177 13 1.6446318218438825e-07\n",
      "52 310311.39758979576 -2631.6643659411347 13 1.6610781400623213e-07\n",
      "53 307668.4474732566 -2642.950116539141 12 1.6776889214629445e-07\n",
      "54 305014.50164582045 -2653.9458274361677 12 1.694465810677574e-07\n",
      "55 302349.10344704054 -2665.398198779905 12 1.71141046878435e-07\n",
      "56 299672.4416036509 -2676.661843389622 12 1.7285245734721935e-07\n",
      "57 296984.71282498667 -2687.7287786642555 12 1.7458098192069155e-07\n",
      "58 294286.3442647031 -2698.3685602835612 11 1.7632679173989847e-07\n",
      "59 291578.18450538523 -2708.159759317874 10 1.7809005965729747e-07\n",
      "60 288860.04601756926 -2718.1384878159733 10 1.7987096025387044e-07\n",
      "61 286131.85097885807 -2728.1950387111865 9 1.8166966985640915e-07\n",
      "62 283394.2718488181 -2737.579130039958 8 1.8348636655497324e-07\n",
      "63 280647.3928754433 -2746.8789733747835 8 1.8532123022052297e-07\n",
      "64 277891.0386503147 -2756.3542251286563 8 1.871744425227282e-07\n",
      "65 275125.4760910282 -2765.5625592864817 8 1.8904618694795546e-07\n",
      "66 272350.982053925 -2774.494037103199 8 1.9093664881743503e-07\n",
      "67 269567.8435795971 -2783.1384743279195 8 1.928460153056094e-07\n",
      "68 266776.3581437857 -2791.4854358113953 8 1.9477447545866548e-07\n",
      "69 263976.8339137832 -2799.524230002484 8 1.9672222021325215e-07\n",
      "70 261170.0104134729 -2806.8235003103036 6 1.9868944241538467e-07\n",
      "71 258356.73650629062 -2813.2739071822725 6 2.0067633683953852e-07\n",
      "72 255536.39769577936 -2820.3388105112535 6 2.0268310020793392e-07\n",
      "73 252709.3633196446 -2827.034376134776 5 2.0470993121001327e-07\n",
      "74 249876.54071309883 -2832.8226065457566 5 2.067570305221134e-07\n",
      "75 247037.73742939945 -2838.8032836993807 5 2.0882460082733453e-07\n",
      "76 244193.3428475081 -2844.3945818913635 5 2.1091284683560788e-07\n",
      "77 241343.75926880917 -2849.583578698919 5 2.1302197530396397e-07\n",
      "78 238489.40222780974 -2854.357040999428 5 2.151521950570036e-07\n",
      "79 235631.08962156242 -2858.3126062473166 4 2.1730371700757364e-07\n",
      "80 232769.22548844738 -2861.864133115043 4 2.194767541776494e-07\n",
      "81 229903.903989135 -2865.321499312384 4 2.2167152171942588e-07\n",
      "82 227035.59629159348 -2868.307697541517 4 2.2388823693662014e-07\n",
      "83 224164.7884408775 -2870.807850715966 4 2.2612711930598634e-07\n",
      "84 221291.98171222376 -2872.8067286537553 4 2.2838839049904622e-07\n",
      "85 218417.91511451214 -2874.066597711615 3 2.3067227440403668e-07\n",
      "86 215543.13357795417 -2874.7815365579736 3 2.3297899714807706e-07\n",
      "87 212667.94056151382 -2875.1930164403457 3 2.3530878711955783e-07\n",
      "88 209792.90151073568 -2875.0390507781412 3 2.3766187499075342e-07\n",
      "89 206918.59897378233 -2874.302536953357 3 2.4003849374066097e-07\n",
      "90 204045.63300256908 -2872.9659712132416 3 2.424388786780676e-07\n",
      "91 201174.6215625347 -2871.0114400343737 3 2.4486326746484827e-07\n",
      "92 198306.42135750374 -2868.2002050309675 2 2.4731190013949676e-07\n",
      "93 195441.74656707037 -2864.674790433375 2 2.4978501914089175e-07\n",
      "94 192580.97385976155 -2860.772707308817 2 2.5228286933230067e-07\n",
      "95 189724.7964879123 -2856.1773718492477 2 2.548056980256237e-07\n",
      "96 186873.92778643407 -2850.8687014782336 2 2.573537550058799e-07\n",
      "97 184029.1016379569 -2844.8261484771792 2 2.5992729255593874e-07\n",
      "98 181191.07294793485 -2838.028690022038 2 2.6252656548149814e-07\n",
      "99 178360.61812992356 -2830.4548180112906 2 2.651518311363131e-07\n",
      "100 175538.5356012371 -2822.082528686471 2 2.6780334944767626e-07\n",
      "101 172725.6462892025 -2812.8893120345892 2 2.70481382942153e-07\n",
      "102 169922.79414822935 -2802.8521409731475 2 2.7318619677157453e-07\n",
      "103 167130.84668791833 -2791.9474603110284 2 2.759180587392903e-07\n",
      "104 164350.6955124376 -2780.1511754807143 2 2.786772393266832e-07\n",
      "105 161583.25687140026 -2767.4386410373554 2 2.8146401171995e-07\n",
      "106 158829.47222247915 -2753.784648921108 2 2.8427865183714953e-07\n",
      "107 156090.30880600392 -2739.163416475232 2 2.8712143835552103e-07\n",
      "108 153366.76023178553 -2723.548574218381 2 2.899926527390762e-07\n",
      "109 150659.84707842272 -2706.9131533628097 2 2.92892579266467e-07\n",
      "110 147970.61750534663 -2689.229573076096 2 2.9582150505913165e-07\n",
      "111 145300.14787786768 -2670.4696274789458 2 2.98779720109723e-07\n",
      "112 142649.54340549256 -2650.6044723751256 2 3.017675173108202e-07\n",
      "113 140019.938793785 -2629.6046117075603 2 3.047851924839284e-07\n",
      "114 137412.49891005005 -2607.4398837349436 2 3.0783304440876767e-07\n",
      "115 134828.4194631267 -2584.079446923366 2 3.109113748528554e-07\n",
      "116 132268.9276975795 -2559.4917655471945 2 3.1402048860138394e-07\n",
      "117 129735.2831025868 -2533.6445949926856 2 3.171606934873978e-07\n",
      "118 127228.7781358271 -2506.504966759705 2 3.2033230042227175e-07\n",
      "119 124750.73896267384 -2478.039173153258 2 3.235356234264945e-07\n",
      "120 122302.52621101288 -2448.2127516609617 2 3.2677097966075946e-07\n",
      "121 119885.53574200495 -2416.9904690079275 2 3.3003868945736705e-07\n",
      "122 117501.19943712099 -2384.3363048839674 2 3.3333907635194074e-07\n",
      "123 115150.98600178541 -2350.2134353355796 2 3.3667246711546013e-07\n",
      "124 112836.40178596886 -2314.584215816547 2 3.400391917866147e-07\n",
      "125 110558.9916220794 -2277.4101638894645 2 3.434395837044809e-07\n",
      "126 108320.33968050739 -2238.6519415720104 2 3.468739795415257e-07\n",
      "127 106122.07034318759 -2198.269337319798 2 3.50342719336941e-07\n",
      "128 103965.84909554815 -2156.2212476394343 2 3.538461465303104e-07\n",
      "129 101853.38343722533 -2112.4656583228207 2 3.573846079956135e-07\n",
      "130 99786.42381192828 -2066.9596252970514 2 3.6095845407556966e-07\n",
      "131 97766.76455684846 -2019.6592550798232 2 3.645680386163254e-07\n",
      "132 95796.24487201404 -1970.5196848344203 2 3.6821371900248867e-07\n",
      "133 93876.7498099997 -1919.4950620143354 2 3.7189585619251353e-07\n",
      "134 92010.21128640899 -1866.538523590716 2 3.7561481475443865e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 90198.60911155588 -1811.6021748531057 2 3.7937096290198303e-07\n",
      "136 88443.97204378064 -1754.637067775242 2 3.8316467253100284e-07\n",
      "137 86748.37886484349 -1695.5931789371534 2 3.8699631925631286e-07\n",
      "138 85113.95947784901 -1634.419386994472 2 3.9086628244887597e-07\n",
      "139 83542.89602816373 -1571.0634496852872 2 3.947749452733647e-07\n",
      "140 82037.42404779734 -1505.4719803663902 2 3.987226947260984e-07\n",
      "141 80599.83362372982 -1437.5904240675154 2 4.027099216733594e-07\n",
      "142 79232.49359843646 -1367.3400252933643 3 4.0673702089009297e-07\n",
      "143 77938.56685966208 -1293.9267387743748 3 4.108043910989939e-07\n",
      "144 76719.99847532173 -1218.568384340353 4 4.1491243500998386e-07\n",
      "145 75579.84634823883 -1140.1521270828962 5 4.190615593600837e-07\n",
      "146 74520.51056654497 -1059.335781693866 5 4.2325217495368454e-07\n",
      "147 73544.3004879401 -976.2100786048686 6 4.2748469670322137e-07\n",
      "148 72654.3935438616 -889.9069440784951 6 4.317595436702536e-07\n",
      "149 71853.57515282535 -800.8183910362568 8 4.3607713910695614e-07\n",
      "150 71145.28873950888 -708.2864133164694 8 4.404379104980257e-07\n",
      "151 70531.8791249828 -613.4096145260701 9 4.44842289603006e-07\n",
      "152 70016.4635984053 -515.4155265775044 9 4.4929071249903604e-07\n",
      "153 69601.7625081143 -414.70109029099694 10 4.537836196240264e-07\n",
      "154 69293.16479804319 -308.59771007111704 11 4.583214558202667e-07\n",
      "155 69095.52029913152 -197.6444989116717 13 4.6290467037846935e-07\n",
      "156 69011.6072402783 -83.91305885321344 15 4.6753371708225403e-07\n",
      "157 69009.68342980507 -1.9238104732357897 16 1.1805226356326915e-07\n",
      "158 69009.68342980507 0.0 16 8.675326986042044e-19\n",
      "LMNN converged with objective 69009.68342980507\n",
      "PCA-LMNN with 50 dimensions:\n",
      "Mean average precision: 0.998096543322674\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7527638190954773\n",
      "Top5-Recall: 0.9974874371859297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 609178.3512521915 -2129.4583602615166 28 1.0099999999999999e-07\n",
      "3 607033.6349467647 -2144.7163054267876 27 1.0201e-07\n",
      "4 604874.8866688842 -2158.7482778804842 27 1.030301e-07\n",
      "5 602700.7334705363 -2174.1531983478926 26 1.0406040099999999e-07\n",
      "6 600512.5939050626 -2188.1395654736552 25 1.0510100500999999e-07\n",
      "7 598310.720187952 -2201.873717110604 24 1.0615201506009999e-07\n",
      "8 596094.3087417948 -2216.411446157261 24 1.0721353521070098e-07\n",
      "9 593862.8092494755 -2231.4994923193008 23 1.08285670562808e-07\n",
      "10 591617.6485438145 -2245.1607056609355 22 1.0936852726843608e-07\n",
      "11 589357.8952005858 -2259.7533432287164 21 1.1046221254112044e-07\n",
      "12 587083.5265098652 -2274.368690720643 20 1.1156683466653164e-07\n",
      "13 584794.8360663985 -2288.6904434666503 19 1.1268250301319696e-07\n",
      "14 582491.7756012398 -2303.0604651587782 19 1.1380932804332893e-07\n",
      "15 580173.0752279967 -2318.700373243075 19 1.1494742132376222e-07\n",
      "16 577838.7271005886 -2334.3481274080696 19 1.1609689553699984e-07\n",
      "17 575488.7262803889 -2350.000820199726 19 1.1725786449236985e-07\n",
      "18 573123.0708253203 -2365.6554550685687 19 1.1843044313729355e-07\n",
      "19 570742.3046276809 -2380.766197639401 18 1.1961474756866649e-07\n",
      "20 568346.5245947266 -2395.780032954295 18 1.2081089504435316e-07\n",
      "21 565935.9983152815 -2410.5262794451555 15 1.220190039947967e-07\n",
      "22 563512.9009509286 -2423.0973643528996 15 1.2323919403474465e-07\n",
      "23 561074.3775184319 -2438.5234324967023 14 1.244715859750921e-07\n",
      "24 558621.2596809567 -2453.117837475147 14 1.2571630183484302e-07\n",
      "25 556152.5699041052 -2468.689776851563 14 1.2697346485319146e-07\n",
      "26 553668.3325491879 -2484.237354917219 14 1.2824319950172337e-07\n",
      "27 551168.5758407484 -2499.756708439556 14 1.295256314967406e-07\n",
      "28 548653.3319779532 -2515.243862795178 14 1.30820887811708e-07\n",
      "29 546123.3236636748 -2530.0083142784424 12 1.321290966898251e-07\n",
      "30 543579.2359042342 -2544.087759440532 12 1.3345038765672335e-07\n",
      "31 541019.9441179746 -2559.2917862596223 11 1.347848915332906e-07\n",
      "32 538446.6785853296 -2573.2655326450476 10 1.361327404486235e-07\n",
      "33 535858.8025891694 -2587.8759961602045 10 1.3749406785310975e-07\n",
      "34 533255.721648999 -2603.08094017033 10 1.3886900853164084e-07\n",
      "35 530637.498259082 -2618.2233899170533 10 1.4025769861695725e-07\n",
      "36 528004.1998509471 -2633.2984081348404 10 1.4166027560312682e-07\n",
      "37 525355.898929448 -2648.300921499147 10 1.430768783591581e-07\n",
      "38 522693.19068086945 -2662.708248578536 9 1.4450764714274967e-07\n",
      "39 520016.2764641767 -2676.914216692734 9 1.4595272361417717e-07\n",
      "40 517324.6062717998 -2691.670192376885 9 1.4741225085031894e-07\n",
      "41 514618.39137106214 -2706.2149007376865 8 1.4888637335882214e-07\n",
      "42 511898.3941625417 -2719.997208520421 8 1.5037523709241035e-07\n",
      "43 509163.939050165 -2734.455112376716 8 1.5187898946333445e-07\n",
      "44 506415.1375704664 -2748.8014796986245 8 1.533977793579678e-07\n",
      "45 503652.1075232671 -2763.0300471992814 8 1.5493175715154747e-07\n",
      "46 500874.9731377806 -2777.1343854864826 8 1.5648107472306294e-07\n",
      "47 498084.09044583776 -2790.882691942854 7 1.5804588547029357e-07\n",
      "48 495280.28036571894 -2803.8100801188266 7 1.5962634432499652e-07\n",
      "49 492462.77381168155 -2817.5065540373907 7 1.6122260776824649e-07\n",
      "50 489632.1171083916 -2830.6567032899475 6 1.6283483384592896e-07\n",
      "51 486788.6511732978 -2843.4659350938164 6 1.6446318218438825e-07\n",
      "52 483931.9615438232 -2856.6896294745966 6 1.6610781400623213e-07\n",
      "53 481062.2217234205 -2869.7398204026977 6 1.6776889214629445e-07\n",
      "54 478179.61309595866 -2882.6086274618283 6 1.694465810677574e-07\n",
      "55 475284.3251280481 -2895.2879679105827 6 1.71141046878435e-07\n",
      "56 472376.55557587626 -2907.7695521718124 6 1.7285245734721935e-07\n",
      "57 469456.51069663843 -2920.04487923783 6 1.7458098192069155e-07\n",
      "58 466524.5523897995 -2931.958306838933 5 1.7632679173989847e-07\n",
      "59 463581.881972667 -2942.6704171324964 4 1.7809005965729747e-07\n",
      "60 460628.0293613139 -2953.852611353097 4 1.7987096025387044e-07\n",
      "61 457662.8059520723 -2965.2234092415893 4 1.8166966985640915e-07\n",
      "62 454686.4632083003 -2976.342743772024 4 1.8348636655497324e-07\n",
      "63 451699.2624479333 -2987.200760366977 4 1.8532123022052297e-07\n",
      "64 448701.4750895644 -2997.7873583689216 4 1.871744425227282e-07\n",
      "65 445693.38290394546 -3008.092185618938 4 1.8904618694795546e-07\n",
      "66 442675.27827103593 -3018.104632909526 4 1.9093664881743503e-07\n",
      "67 439647.4644427017 -3027.8138283342123 4 1.928460153056094e-07\n",
      "68 436610.2558111919 -3037.2086315097986 4 1.9477447545866548e-07\n",
      "69 433563.97818350873 -3046.2776276831864 4 1.9672222021325215e-07\n",
      "70 430508.9690618048 -3055.0091217039153 4 1.9868944241538467e-07\n",
      "71 427445.577929912 -3063.3911318928003 4 2.0067633683953852e-07\n",
      "72 424374.5354687928 -3071.042461119243 3 2.0268310020793392e-07\n",
      "73 421296.2083042556 -3078.327164537157 3 2.0470993121001327e-07\n",
      "74 418210.6157345467 -3085.5925697089406 3 2.067570305221134e-07\n",
      "75 415118.1578147318 -3092.4579198148567 3 2.0882460082733453e-07\n",
      "76 412019.24810188025 -3098.9097128515714 3 2.1091284683560788e-07\n",
      "77 408914.3139810614 -3104.934120818856 3 2.1302197530396397e-07\n",
      "78 405803.7969984489 -3110.516982612491 3 2.151521950570036e-07\n",
      "79 402688.15320166084 -3115.643796788063 3 2.1730371700757364e-07\n",
      "80 399567.85348750436 -3120.2997141564847 3 2.194767541776494e-07\n",
      "81 396443.3839572687 -3124.4695302356267 3 2.2167152171942588e-07\n",
      "82 393315.24627973454 -3128.1376775341923 3 2.2388823693662014e-07\n",
      "83 390183.9580620416 -3131.288217692927 3 2.2612711930598634e-07\n",
      "84 387050.0536737449 -3133.904388296709 2 2.2838839049904622e-07\n",
      "85 383914.83741543314 -3135.2162583117606 2 2.3067227440403668e-07\n",
      "86 380778.10916650156 -3136.728248931584 2 2.3297899714807706e-07\n",
      "87 377640.4535945002 -3137.6555720013566 2 2.3530878711955783e-07\n",
      "88 374502.47326938325 -3137.9803251169506 2 2.3766187499075342e-07\n",
      "89 371364.78908485116 -3137.6841845320887 2 2.4003849374066097e-07\n",
      "90 368228.0406887819 -3136.748396069277 2 2.424388786780676e-07\n",
      "91 365092.88692292874 -3135.153765853145 2 2.4486326746484827e-07\n",
      "92 361960.00627208763 -3132.880650841107 2 2.4731190013949676e-07\n",
      "93 358830.1171949004 -3129.8890771872248 1 2.4978501914089175e-07\n",
      "94 355704.0433629284 -3126.073831972026 1 2.5228286933230067e-07\n",
      "95 352582.4028352764 -3121.6405276519945 1 2.548056980256237e-07\n",
      "96 349465.95739524334 -3116.4454400330433 1 2.573537550058799e-07\n",
      "97 346355.49086471327 -3110.466530530073 1 2.5992729255593874e-07\n",
      "98 343251.82605059654 -3103.6648141167243 0 2.6252656548149814e-07\n",
      "99 340155.8880552419 -3095.937995354645 0 2.651518311363131e-07\n",
      "100 337068.4211985244 -3087.4668567174813 0 2.6780334944767626e-07\n",
      "101 333990.3027452545 -3078.118453269941 0 2.70481382942153e-07\n",
      "102 330922.43466321373 -3067.8680820407462 0 2.7318619677157453e-07\n",
      "103 327865.744190386 -3056.6904728277586 0 2.759180587392903e-07\n",
      "104 324821.1844142924 -3044.559776093578 0 2.786772393266832e-07\n",
      "105 321789.73486368934 -3031.4495506030507 0 2.8146401171995e-07\n",
      "106 318772.40211288014 -3017.3327508092043 0 2.8427865183714953e-07\n",
      "107 315770.22039889765 -3002.1817139824852 0 2.8712143835552103e-07\n",
      "108 312784.25225183327 -2985.9681470643845 0 2.899926527390762e-07\n",
      "109 309815.58913857583 -2968.6631132574403 0 2.92892579266467e-07\n",
      "110 306865.3521202402 -2950.2370183356106 0 2.9582150505913165e-07\n",
      "111 303934.69252356986 -2930.6595966703608 0 2.98779720109723e-07\n",
      "112 301024.7926265971 -2909.8998969727545 0 3.017675173108202e-07\n",
      "113 298136.8663588621 -2887.92626773502 0 3.047851924839284e-07\n",
      "114 295272.16001648403 -2864.706342378049 0 3.0783304440876767e-07\n",
      "115 292431.9529923958 -2840.207024088246 0 3.109113748528554e-07\n",
      "116 289617.5585220531 -2814.394470342668 0 3.1402048860138394e-07\n",
      "117 286830.32444493537 -2787.234077117755 0 3.171606934873978e-07\n",
      "118 284071.63398216694 -2758.690462768427 0 3.2033230042227175e-07\n",
      "119 281342.90653058985 -2728.727451577084 0 3.235356234264945e-07\n",
      "120 278645.5984736236 -2697.308056966227 0 3.2677097966075946e-07\n",
      "121 275981.2040092635 -2664.3944643601426 0 3.3003868945736705e-07\n",
      "122 273351.25599556684 -2629.948013696645 0 3.3333907635194074e-07\n",
      "123 270757.3268139857 -2593.9291815811303 0 3.3667246711546013e-07\n",
      "124 268201.0292509175 -2556.2975630682195 0 3.400391917866147e-07\n",
      "125 265684.0173978452 -2517.0118530722684 0 3.434395837044809e-07\n",
      "126 263207.98757044994 -2476.0298273952794 0 3.468739795415257e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 260774.67924708658 -2433.308323363366 0 3.50342719336941e-07\n",
      "128 258385.87602702188 -2388.8032200646994 0 3.538461465303104e-07\n",
      "129 256043.4066088386 -2342.4694181832892 0 3.573846079956135e-07\n",
      "130 253749.14578942006 -2294.2608194185304 0 3.6095845407556966e-07\n",
      "131 251505.01548394258 -2244.130305477476 0 3.645680386163254e-07\n",
      "132 249312.98576730047 -2192.029716642108 0 3.6821371900248867e-07\n",
      "133 247175.07593740843 -2137.9098298920435 0 3.7189585619251353e-07\n",
      "134 245093.3556008316 -2081.7203365768364 0 3.7561481475443865e-07\n",
      "135 243069.945781194 -2023.4098196375999 0 3.7937096290198303e-07\n",
      "136 241107.020050843 -1962.9257303509803 0 3.8316467253100284e-07\n",
      "137 239206.80568624026 -1900.2143646027544 0 3.8699631925631286e-07\n",
      "138 237371.58484756664 -1835.2208386736165 0 3.9086628244887597e-07\n",
      "139 235603.69578303955 -1767.8890645270876 0 3.947749452733647e-07\n",
      "140 233905.53405844973 -1698.1617245898233 0 3.987226947260984e-07\n",
      "141 232279.553812431 -1625.9802460187348 0 4.027099216733594e-07\n",
      "142 230728.26903799525 -1551.2847744357423 0 4.0673702089009297e-07\n",
      "143 229254.25489086623 -1474.0141471290262 0 4.108043910989939e-07\n",
      "144 227860.14902516588 -1394.105865700345 0 4.1491243500998386e-07\n",
      "145 226548.6529570086 -1311.4960681572848 0 4.190615593600837e-07\n",
      "146 225322.53345657926 -1226.1195004293404 0 4.2325217495368454e-07\n",
      "147 224184.6239692765 -1137.9094873027643 0 4.2748469670322137e-07\n",
      "148 223137.82606651477 -1046.7979027617257 0 4.317595436702536e-07\n",
      "149 222185.11092679593 -952.7151397188427 0 4.3607713910695614e-07\n",
      "150 221329.5208476661 -855.5900791298191 0 4.404379104980257e-07\n",
      "151 220574.17078919167 -755.3500584744324 0 4.44842289603006e-07\n",
      "152 219922.24994960043 -651.9208395912428 0 4.4929071249903604e-07\n",
      "153 219377.02337374308 -545.2265758573485 0 4.537836196240264e-07\n",
      "154 218941.8335950459 -435.1897786971822 0 4.583214558202667e-07\n",
      "155 218620.10231164534 -321.731283400557 0 4.6290467037846935e-07\n",
      "156 218415.3320973958 -204.77021424955456 0 4.6753371708225403e-07\n",
      "157 218331.12212660696 -84.20997078882647 1 4.722090542530766e-07\n",
      "158 218329.69307770563 -1.4290489013365004 1 1.1923278619890185e-07\n",
      "159 218329.69307770563 0.0 1 4.381040127951233e-19\n",
      "LMNN converged with objective 218329.69307770563\n",
      "PCA-LMNN with 500 dimensions:\n",
      "Mean average precision: 0.999725902238465\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "LargeMarg_Model = LMNN(verbose=True).fit(train_noval_feat_PCA_50,train_noval_labels)\n",
    "val_query_feat_LMNN = LargeMarg_Model.transform(val_query_feat_PCA_50)\n",
    "val_gallery_feat_LMNN = LargeMarg_Model.transform(val_gallery_feat_PCA_50)\n",
    "print(\"PCA-LMNN with 50 dimensions:\")\n",
    "kNN_val(val_query_feat_LMNN,val_gallery_feat_LMNN)\n",
    "\n",
    "LargeMarg_Model = LMNN(verbose=True).fit(train_noval_feat_PCA_500,train_noval_labels)\n",
    "val_query_feat_LMNN = LargeMarg_Model.transform(val_query_feat_PCA_500)\n",
    "val_gallery_feat_LMNN = LargeMarg_Model.transform(val_gallery_feat_PCA_500)\n",
    "print(\"PCA-LMNN with 500 dimensions:\")\n",
    "kNN_val(val_query_feat_LMNN,val_gallery_feat_LMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on number of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 426205.3505330503 -1986.03270395234 46 1.0099999999999999e-07\n",
      "3 424209.49658753513 -1995.8539455151767 46 1.0201e-07\n",
      "4 422199.46581847203 -2010.0307690631016 46 1.030301e-07\n",
      "5 420175.5792956003 -2023.8865228717332 45 1.0406040099999999e-07\n",
      "6 418139.40073424025 -2036.1785613600514 44 1.0510100500999999e-07\n",
      "7 416089.97406359215 -2049.4266706480994 44 1.0615201506009999e-07\n",
      "8 414026.41549335717 -2063.558570234978 43 1.0721353521070098e-07\n",
      "9 411950.9807652671 -2075.4347280900693 41 1.08285670562808e-07\n",
      "10 409864.33312955755 -2086.6476357095526 39 1.0936852726843608e-07\n",
      "11 407764.9992526966 -2099.3338768609683 38 1.1046221254112044e-07\n",
      "12 405652.3872425703 -2112.612010126293 35 1.1156683466653164e-07\n",
      "13 403528.99305547157 -2123.3941870987182 35 1.1268250301319696e-07\n",
      "14 401391.2683750809 -2137.7246803906746 34 1.1380932804332893e-07\n",
      "15 399240.5115192417 -2150.756855839165 34 1.1494742132376222e-07\n",
      "16 397075.40797966864 -2165.103539573087 34 1.1609689553699984e-07\n",
      "17 394895.9582233088 -2179.449756359856 34 1.1725786449236985e-07\n",
      "18 392702.1656148487 -2193.792608460062 34 1.1843044313729355e-07\n",
      "19 390494.6752672554 -2207.4903475933243 33 1.1961474756866649e-07\n",
      "20 388274.09567424597 -2220.579593009432 31 1.2081089504435316e-07\n",
      "21 386041.41978231503 -2232.675891930936 30 1.220190039947967e-07\n",
      "22 383796.0590217676 -2245.3607605474535 29 1.2323919403474465e-07\n",
      "23 381536.913034405 -2259.145987362601 28 1.244715859750921e-07\n",
      "24 379264.583967846 -2272.329066558974 27 1.2571630183484302e-07\n",
      "25 376979.1184908802 -2285.4654769658227 27 1.2697346485319146e-07\n",
      "26 374679.59935199644 -2299.519138883741 26 1.2824319950172337e-07\n",
      "27 372366.9077758147 -2312.6915761817363 26 1.295256314967406e-07\n",
      "28 370040.0631426525 -2326.8446331621963 26 1.30820887811708e-07\n",
      "29 367699.3453371789 -2340.7178054735996 25 1.321290966898251e-07\n",
      "30 365345.3391646182 -2354.0061725606793 25 1.3345038765672335e-07\n",
      "31 362977.621581228 -2367.717583390244 24 1.347848915332906e-07\n",
      "32 360596.93786643655 -2380.683714791434 24 1.361327404486235e-07\n",
      "33 358203.8520845282 -2393.0857819083612 20 1.3749406785310975e-07\n",
      "34 355800.0756027894 -2403.7764817387797 20 1.3886900853164084e-07\n",
      "35 353382.89527429745 -2417.180328491959 19 1.4025769861695725e-07\n",
      "36 350952.4390215634 -2430.4562527340604 19 1.4166027560312682e-07\n",
      "37 348508.306005888 -2444.133015675412 19 1.430768783591581e-07\n",
      "38 346050.57307841367 -2457.7329274743097 19 1.4450764714274967e-07\n",
      "39 343579.32215253136 -2471.250925882312 19 1.4595272361417717e-07\n",
      "40 341094.64034080756 -2484.6818117238 19 1.4741225085031894e-07\n",
      "41 338596.7585817326 -2497.8817590749823 18 1.4888637335882214e-07\n",
      "42 336086.6324241602 -2510.1261575723765 17 1.5037523709241035e-07\n",
      "43 333564.6243715825 -2522.008052577672 16 1.5187898946333445e-07\n",
      "44 331030.33795827103 -2534.2864133114927 16 1.533977793579678e-07\n",
      "45 328483.1236171707 -2547.2143411003053 16 1.5493175715154747e-07\n",
      "46 325923.2850968088 -2559.8385203619255 15 1.5648107472306294e-07\n",
      "47 323351.2672700155 -2572.017826793308 15 1.5804588547029357e-07\n",
      "48 320766.912602143 -2584.3546678724815 14 1.5962634432499652e-07\n",
      "49 318170.75150503137 -2596.161097111646 14 1.6122260776824649e-07\n",
      "50 315562.7613810331 -2607.9901239982573 13 1.6283483384592896e-07\n",
      "51 312943.0619557369 -2619.6994252962177 13 1.6446318218438825e-07\n",
      "52 310311.39758979576 -2631.6643659411347 13 1.6610781400623213e-07\n",
      "53 307668.4474732566 -2642.950116539141 12 1.6776889214629445e-07\n",
      "54 305014.50164582045 -2653.9458274361677 12 1.694465810677574e-07\n",
      "55 302349.10344704054 -2665.398198779905 12 1.71141046878435e-07\n",
      "56 299672.4416036509 -2676.661843389622 12 1.7285245734721935e-07\n",
      "57 296984.71282498667 -2687.7287786642555 12 1.7458098192069155e-07\n",
      "58 294286.3442647031 -2698.3685602835612 11 1.7632679173989847e-07\n",
      "59 291578.18450538523 -2708.159759317874 10 1.7809005965729747e-07\n",
      "60 288860.04601756926 -2718.1384878159733 10 1.7987096025387044e-07\n",
      "61 286131.85097885807 -2728.1950387111865 9 1.8166966985640915e-07\n",
      "62 283394.2718488181 -2737.579130039958 8 1.8348636655497324e-07\n",
      "63 280647.3928754433 -2746.8789733747835 8 1.8532123022052297e-07\n",
      "64 277891.0386503147 -2756.3542251286563 8 1.871744425227282e-07\n",
      "65 275125.4760910282 -2765.5625592864817 8 1.8904618694795546e-07\n",
      "66 272350.982053925 -2774.494037103199 8 1.9093664881743503e-07\n",
      "67 269567.8435795971 -2783.1384743279195 8 1.928460153056094e-07\n",
      "68 266776.3581437857 -2791.4854358113953 8 1.9477447545866548e-07\n",
      "69 263976.8339137832 -2799.524230002484 8 1.9672222021325215e-07\n",
      "70 261170.0104134729 -2806.8235003103036 6 1.9868944241538467e-07\n",
      "71 258356.73650629062 -2813.2739071822725 6 2.0067633683953852e-07\n",
      "72 255536.39769577936 -2820.3388105112535 6 2.0268310020793392e-07\n",
      "73 252709.3633196446 -2827.034376134776 5 2.0470993121001327e-07\n",
      "74 249876.54071309883 -2832.8226065457566 5 2.067570305221134e-07\n",
      "75 247037.73742939945 -2838.8032836993807 5 2.0882460082733453e-07\n",
      "76 244193.3428475081 -2844.3945818913635 5 2.1091284683560788e-07\n",
      "77 241343.75926880917 -2849.583578698919 5 2.1302197530396397e-07\n",
      "78 238489.40222780974 -2854.357040999428 5 2.151521950570036e-07\n",
      "79 235631.08962156242 -2858.3126062473166 4 2.1730371700757364e-07\n",
      "80 232769.22548844738 -2861.864133115043 4 2.194767541776494e-07\n",
      "81 229903.903989135 -2865.321499312384 4 2.2167152171942588e-07\n",
      "82 227035.59629159348 -2868.307697541517 4 2.2388823693662014e-07\n",
      "83 224164.7884408775 -2870.807850715966 4 2.2612711930598634e-07\n",
      "84 221291.98171222376 -2872.8067286537553 4 2.2838839049904622e-07\n",
      "85 218417.91511451214 -2874.066597711615 3 2.3067227440403668e-07\n",
      "86 215543.13357795417 -2874.7815365579736 3 2.3297899714807706e-07\n",
      "87 212667.94056151382 -2875.1930164403457 3 2.3530878711955783e-07\n",
      "88 209792.90151073568 -2875.0390507781412 3 2.3766187499075342e-07\n",
      "89 206918.59897378233 -2874.302536953357 3 2.4003849374066097e-07\n",
      "90 204045.63300256908 -2872.9659712132416 3 2.424388786780676e-07\n",
      "91 201174.6215625347 -2871.0114400343737 3 2.4486326746484827e-07\n",
      "92 198306.42135750374 -2868.2002050309675 2 2.4731190013949676e-07\n",
      "93 195441.74656707037 -2864.674790433375 2 2.4978501914089175e-07\n",
      "94 192580.97385976155 -2860.772707308817 2 2.5228286933230067e-07\n",
      "95 189724.7964879123 -2856.1773718492477 2 2.548056980256237e-07\n",
      "96 186873.92778643407 -2850.8687014782336 2 2.573537550058799e-07\n",
      "97 184029.1016379569 -2844.8261484771792 2 2.5992729255593874e-07\n",
      "98 181191.07294793485 -2838.028690022038 2 2.6252656548149814e-07\n",
      "99 178360.61812992356 -2830.4548180112906 2 2.651518311363131e-07\n",
      "100 175538.5356012371 -2822.082528686471 2 2.6780334944767626e-07\n",
      "101 172725.6462892025 -2812.8893120345892 2 2.70481382942153e-07\n",
      "102 169922.79414822935 -2802.8521409731475 2 2.7318619677157453e-07\n",
      "103 167130.84668791833 -2791.9474603110284 2 2.759180587392903e-07\n",
      "104 164350.6955124376 -2780.1511754807143 2 2.786772393266832e-07\n",
      "105 161583.25687140026 -2767.4386410373554 2 2.8146401171995e-07\n",
      "106 158829.47222247915 -2753.784648921108 2 2.8427865183714953e-07\n",
      "107 156090.30880600392 -2739.163416475232 2 2.8712143835552103e-07\n",
      "108 153366.76023178553 -2723.548574218381 2 2.899926527390762e-07\n",
      "109 150659.84707842272 -2706.9131533628097 2 2.92892579266467e-07\n",
      "110 147970.61750534663 -2689.229573076096 2 2.9582150505913165e-07\n",
      "111 145300.14787786768 -2670.4696274789458 2 2.98779720109723e-07\n",
      "112 142649.54340549256 -2650.6044723751256 2 3.017675173108202e-07\n",
      "113 140019.938793785 -2629.6046117075603 2 3.047851924839284e-07\n",
      "114 137412.49891005005 -2607.4398837349436 2 3.0783304440876767e-07\n",
      "115 134828.4194631267 -2584.079446923366 2 3.109113748528554e-07\n",
      "116 132268.9276975795 -2559.4917655471945 2 3.1402048860138394e-07\n",
      "117 129735.2831025868 -2533.6445949926856 2 3.171606934873978e-07\n",
      "118 127228.7781358271 -2506.504966759705 2 3.2033230042227175e-07\n",
      "119 124750.73896267384 -2478.039173153258 2 3.235356234264945e-07\n",
      "120 122302.52621101288 -2448.2127516609617 2 3.2677097966075946e-07\n",
      "121 119885.53574200495 -2416.9904690079275 2 3.3003868945736705e-07\n",
      "122 117501.19943712099 -2384.3363048839674 2 3.3333907635194074e-07\n",
      "123 115150.98600178541 -2350.2134353355796 2 3.3667246711546013e-07\n",
      "124 112836.40178596886 -2314.584215816547 2 3.400391917866147e-07\n",
      "125 110558.9916220794 -2277.4101638894645 2 3.434395837044809e-07\n",
      "126 108320.33968050739 -2238.6519415720104 2 3.468739795415257e-07\n",
      "127 106122.07034318759 -2198.269337319798 2 3.50342719336941e-07\n",
      "128 103965.84909554815 -2156.2212476394343 2 3.538461465303104e-07\n",
      "129 101853.38343722533 -2112.4656583228207 2 3.573846079956135e-07\n",
      "130 99786.42381192828 -2066.9596252970514 2 3.6095845407556966e-07\n",
      "131 97766.76455684846 -2019.6592550798232 2 3.645680386163254e-07\n",
      "132 95796.24487201404 -1970.5196848344203 2 3.6821371900248867e-07\n",
      "133 93876.7498099997 -1919.4950620143354 2 3.7189585619251353e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 92010.21128640899 -1866.538523590716 2 3.7561481475443865e-07\n",
      "135 90198.60911155588 -1811.6021748531057 2 3.7937096290198303e-07\n",
      "136 88443.97204378064 -1754.637067775242 2 3.8316467253100284e-07\n",
      "137 86748.37886484349 -1695.5931789371534 2 3.8699631925631286e-07\n",
      "138 85113.95947784901 -1634.419386994472 2 3.9086628244887597e-07\n",
      "139 83542.89602816373 -1571.0634496852872 2 3.947749452733647e-07\n",
      "140 82037.42404779734 -1505.4719803663902 2 3.987226947260984e-07\n",
      "141 80599.83362372982 -1437.5904240675154 2 4.027099216733594e-07\n",
      "142 79232.49359843646 -1367.3400252933643 3 4.0673702089009297e-07\n",
      "143 77938.56685966208 -1293.9267387743748 3 4.108043910989939e-07\n",
      "144 76719.99847532173 -1218.568384340353 4 4.1491243500998386e-07\n",
      "145 75579.84634823883 -1140.1521270828962 5 4.190615593600837e-07\n",
      "146 74520.51056654497 -1059.335781693866 5 4.2325217495368454e-07\n",
      "147 73544.3004879401 -976.2100786048686 6 4.2748469670322137e-07\n",
      "148 72654.3935438616 -889.9069440784951 6 4.317595436702536e-07\n",
      "149 71853.57515282535 -800.8183910362568 8 4.3607713910695614e-07\n",
      "150 71145.28873950888 -708.2864133164694 8 4.404379104980257e-07\n",
      "151 70531.8791249828 -613.4096145260701 9 4.44842289603006e-07\n",
      "152 70016.4635984053 -515.4155265775044 9 4.4929071249903604e-07\n",
      "153 69601.7625081143 -414.70109029099694 10 4.537836196240264e-07\n",
      "154 69293.16479804319 -308.59771007111704 11 4.583214558202667e-07\n",
      "155 69095.52029913152 -197.6444989116717 13 4.6290467037846935e-07\n",
      "156 69011.6072402783 -83.91305885321344 15 4.6753371708225403e-07\n",
      "157 69009.68342980507 -1.9238104732357897 16 1.1805226356326915e-07\n",
      "158 69009.68342980507 0.0 16 8.675326986042044e-19\n",
      "LMNN converged with objective 69009.68342980507\n",
      "PCA-LMNN with 50 dimensions and k=3:\n",
      "Mean average precision: 0.998096543322674\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7527638190954773\n",
      "Top5-Recall: 0.9974874371859297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 910478.4096892874 -10163.797583322506 287 1.0099999999999999e-07\n",
      "3 900517.9236205311 -9960.48606875632 261 1.0201e-07\n",
      "4 890692.242275899 -9825.68134463206 248 1.030301e-07\n",
      "5 880910.2741309415 -9781.968144957558 237 1.0406040099999999e-07\n",
      "6 871161.5942398065 -9748.679891134962 221 1.0510100500999999e-07\n",
      "7 861482.5568773566 -9679.037362449919 203 1.0615201506009999e-07\n",
      "8 851858.4010087979 -9624.155868558679 187 1.0721353521070098e-07\n",
      "9 842280.7987418835 -9577.602266914444 175 1.08285670562808e-07\n",
      "10 832734.9429841193 -9545.855757764191 156 1.0936852726843608e-07\n",
      "11 823233.1024048945 -9501.84057922475 143 1.1046221254112044e-07\n",
      "12 813748.7262603714 -9484.37614452315 138 1.1156683466653164e-07\n",
      "13 804258.7271595213 -9489.99910085008 125 1.1268250301319696e-07\n",
      "14 794778.6536658704 -9480.073493650882 120 1.1380932804332893e-07\n",
      "15 785295.3828440292 -9483.2708218412 111 1.1494742132376222e-07\n",
      "16 775812.0523108959 -9483.330533133354 106 1.1609689553699984e-07\n",
      "17 766319.6849910916 -9492.367319804267 99 1.1725786449236985e-07\n",
      "18 756827.813023211 -9491.871967880637 94 1.1843044313729355e-07\n",
      "19 747329.9386217286 -9497.874401482404 90 1.1961474756866649e-07\n",
      "20 737826.092778154 -9503.845843574614 86 1.2081089504435316e-07\n",
      "21 728312.6780352686 -9513.41474288539 80 1.220190039947967e-07\n",
      "22 718794.9360851344 -9517.741950134165 76 1.2323919403474465e-07\n",
      "23 709276.3351627686 -9518.60092236579 67 1.244715859750921e-07\n",
      "24 699762.0014518716 -9514.333710897015 65 1.2571630183484302e-07\n",
      "25 690240.0427105315 -9521.958741340088 60 1.2697346485319146e-07\n",
      "26 680716.6020043442 -9523.440706187277 58 1.2824319950172337e-07\n",
      "27 671188.9737510103 -9527.628253333969 55 1.295256314967406e-07\n",
      "28 661663.1196571549 -9525.854093855363 50 1.30820887811708e-07\n",
      "29 652139.2145619539 -9523.905095200986 48 1.321290966898251e-07\n",
      "30 642614.6312755742 -9524.583286379697 45 1.3345038765672335e-07\n",
      "31 633094.4081449796 -9520.22313059459 42 1.347848915332906e-07\n",
      "32 623577.8336634638 -9516.574481515796 38 1.361327404486235e-07\n",
      "33 614070.8080682422 -9507.025595221668 34 1.3749406785310975e-07\n",
      "34 604571.3659476433 -9499.442120598862 33 1.3886900853164084e-07\n",
      "35 595077.8558804514 -9493.510067191906 33 1.4025769861695725e-07\n",
      "36 585592.6206566553 -9485.235223796102 30 1.4166027560312682e-07\n",
      "37 576122.4314685678 -9470.189188087475 27 1.430768783591581e-07\n",
      "38 566666.6866875743 -9455.744780993555 27 1.4450764714274967e-07\n",
      "39 557224.2808998768 -9442.405787697411 27 1.4595272361417717e-07\n",
      "40 547797.7074220302 -9426.573477846687 26 1.4741225085031894e-07\n",
      "41 538389.6635008334 -9408.043921196717 26 1.4888637335882214e-07\n",
      "42 529002.9360067754 -9386.727494058083 24 1.5037523709241035e-07\n",
      "43 519641.56945096806 -9361.366555807297 24 1.5187898946333445e-07\n",
      "44 510305.66819023626 -9335.9012607318 23 1.533977793579678e-07\n",
      "45 501000.1901333004 -9305.478056935885 20 1.5493175715154747e-07\n",
      "46 491728.7011651234 -9271.488968177 18 1.5648107472306294e-07\n",
      "47 482492.61801453633 -9236.083150587045 18 1.5804588547029357e-07\n",
      "48 473293.85954055254 -9198.758473983791 16 1.5962634432499652e-07\n",
      "49 464136.489178726 -9157.370361826557 15 1.6122260776824649e-07\n",
      "50 455022.02036019426 -9114.46881853172 15 1.6283483384592896e-07\n",
      "51 445953.152771328 -9068.86758886627 14 1.6446318218438825e-07\n",
      "52 436934.2890807048 -9018.863690623199 13 1.6610781400623213e-07\n",
      "53 427968.20305719564 -8966.086023509153 12 1.6776889214629445e-07\n",
      "54 419057.65927418997 -8910.543783005676 12 1.694465810677574e-07\n",
      "55 410205.27789561125 -8852.38137857872 12 1.71141046878435e-07\n",
      "56 401414.77074296324 -8790.507152648002 11 1.7285245734721935e-07\n",
      "57 392690.08033890557 -8724.690404057677 11 1.7458098192069155e-07\n",
      "58 384034.43061704445 -8655.649721861118 11 1.7632679173989847e-07\n",
      "59 375452.1859106129 -8582.24470643152 10 1.7809005965729747e-07\n",
      "60 366947.3724013395 -8504.813509273401 10 1.7987096025387044e-07\n",
      "61 358523.55905327114 -8423.81334806839 8 1.8166966985640915e-07\n",
      "62 350184.9246478891 -8338.634405382036 8 1.8348636655497324e-07\n",
      "63 341935.42862215417 -8249.49602573493 8 1.8532123022052297e-07\n",
      "64 333779.43010680884 -8155.998515345331 8 1.871744425227282e-07\n",
      "65 325721.40152561717 -8058.028581191669 8 1.8904618694795546e-07\n",
      "66 317765.93112929875 -7955.470396318415 8 1.9093664881743503e-07\n",
      "67 309917.725582963 -7848.205546335783 8 1.928460153056094e-07\n",
      "68 302181.6126081442 -7736.112974818796 8 1.9477447545866548e-07\n",
      "69 294562.67291871115 -7618.9396894330275 7 1.9672222021325215e-07\n",
      "70 287066.7625679178 -7495.910350793332 6 1.9868944241538467e-07\n",
      "71 279698.36575303064 -7368.396814887179 6 2.0067633683953852e-07\n",
      "72 272462.57224470726 -7235.793508323375 6 2.0268310020793392e-07\n",
      "73 265364.854450235 -7097.717794472235 6 2.0470993121001327e-07\n",
      "74 258410.82284309607 -6954.031607138953 6 2.067570305221134e-07\n",
      "75 251606.22901880578 -6804.593824290292 6 2.0882460082733453e-07\n",
      "76 244956.96881498097 -6649.260203824815 6 2.1091284683560788e-07\n",
      "77 238469.0854969563 -6487.883318024658 6 2.1302197530396397e-07\n",
      "78 232148.7730102953 -6320.312486661016 6 2.151521950570036e-07\n",
      "79 226002.37930156902 -6146.393708726275 6 2.1730371700757364e-07\n",
      "80 220036.40970880302 -5965.969592765992 6 2.194767541776494e-07\n",
      "81 214257.53042302318 -5778.87928577984 6 2.2167152171942588e-07\n",
      "82 208672.572022358 -5584.958400665171 6 2.2388823693662014e-07\n",
      "83 203288.53308018582 -5384.038942172192 6 2.2612711930598634e-07\n",
      "84 198112.5838488458 -5175.949231340026 6 2.2838839049904622e-07\n",
      "85 193152.0700204614 -4960.513828384399 6 2.3067227440403668e-07\n",
      "86 188414.51656645708 -4737.553454004315 6 2.3297899714807706e-07\n",
      "87 183907.63165738116 -4506.884909075918 6 2.3530878711955783e-07\n",
      "88 179639.34847082593 -4268.283186555229 7 2.3766187499075342e-07\n",
      "89 175617.74181120167 -4021.60665962426 7 2.4003849374066097e-07\n",
      "90 171851.04401624043 -3766.6977949612483 7 2.424388786780676e-07\n",
      "91 168347.7788595209 -3503.2651567195135 8 2.4486326746484827e-07\n",
      "92 165117.91100007592 -3229.8678594449884 11 2.4731190013949676e-07\n",
      "93 162174.99492969783 -2942.9160703780944 14 2.4978501914089175e-07\n",
      "94 159526.6922200524 -2648.302709645417 17 2.5228286933230067e-07\n",
      "95 157182.56804515352 -2344.1241748988978 20 2.548056980256237e-07\n",
      "96 155158.4765608228 -2024.091484330711 27 2.573537550058799e-07\n",
      "97 153470.86643138243 -1687.610129440378 40 2.5992729255593874e-07\n",
      "98 152146.49044001228 -1324.3759913701506 50 2.6252656548149814e-07\n",
      "99 151192.93625506057 -953.5541849517031 62 2.651518311363131e-07\n",
      "100 150641.16055589024 -551.7756991703354 79 2.6780334944767626e-07\n",
      "101 150526.4938234818 -114.66673240842647 99 2.70481382942153e-07\n",
      "102 150526.4938234818 0.0 99 3.105767482015546e-20\n",
      "LMNN converged with objective 150526.4938234818\n",
      "PCA-LMNN with 50 dimensions and k=5:\n",
      "Mean average precision: 0.9984989884487372\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7527638190954773\n",
      "Top5-Recall: 0.9974874371859297\n"
     ]
    }
   ],
   "source": [
    "for k in [3,5]:\n",
    "    LargeMarg_Model = LMNN(verbose=True, k=k).fit(train_noval_feat_PCA_50,train_noval_labels)\n",
    "    val_query_feat_LMNN = LargeMarg_Model.transform(val_query_feat_PCA_50)\n",
    "    val_gallery_feat_LMNN = LargeMarg_Model.transform(val_gallery_feat_PCA_50)\n",
    "    print(\"PCA-LMNN with 50 dimensions and k={}:\".format(k))\n",
    "    kNN_val(val_query_feat_LMNN,val_gallery_feat_LMNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with reduced features - PCA 500 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-LMNN with 500 dimensions and k=3:\n",
      "Mean average precision: 0.40409753143681715\n",
      "Top1: 0.4407142857142857\n",
      "Top5: 0.6407142857142857\n",
      "Top10: 0.735\n",
      "Top5-Precision: 0.3181428571428571\n",
      "Top5-Recall: 0.41928571428571426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-LMNN with 500 dimensions and k=5:\n",
      "Mean average precision: 0.4105104102246959\n",
      "Top1: 0.45285714285714285\n",
      "Top5: 0.6578571428571428\n",
      "Top10: 0.7364285714285714\n",
      "Top5-Precision: 0.32228571428571434\n",
      "Top5-Recall: 0.4245833333333333\n"
     ]
    }
   ],
   "source": [
    "for k in [3,5]:\n",
    "    LargeMarg_Model = LMNN(k=k).fit(train_feat_PCA_500,train_labels)\n",
    "    query_feat_LMNN = LargeMarg_Model.transform(query_feat_PCA_500)\n",
    "    gallery_feat_LMNN = LargeMarg_Model.transform(gallery_feat_PCA_500)\n",
    "    print(\"PCA-LMNN with 500 dimensions and k={}:\".format(k))\n",
    "    kNN(query_feat_LMNN,gallery_feat_LMNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test  with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulstreli/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 765604.840951463 -2816.7135545150377 28 1.0099999999999999e-07\n",
      "3 762768.9081833565 -2835.9327681064606 27 1.0201e-07\n",
      "4 759915.330559735 -2853.5776236215606 27 1.030301e-07\n",
      "5 757042.9118664921 -2872.4186932428274 26 1.0406040099999999e-07\n",
      "6 754152.8838508255 -2890.0280156666413 25 1.0510100500999999e-07\n",
      "7 751245.2307615664 -2907.6530892590526 24 1.0615201506009999e-07\n",
      "8 748319.10616948 -2926.1245920864167 24 1.0721353521070098e-07\n",
      "9 745375.0063233762 -2944.0998461038107 22 1.08285670562808e-07\n",
      "10 742414.2419775826 -2960.7643457936356 21 1.0936852726843608e-07\n",
      "11 739435.3387439473 -2978.9032336352393 21 1.1046221254112044e-07\n",
      "12 736437.7864819139 -2997.5522620334523 19 1.1156683466653164e-07\n",
      "13 733422.8023871669 -3014.984094746993 19 1.1268250301319696e-07\n",
      "14 730388.6561803279 -3034.14620683901 19 1.1380932804332893e-07\n",
      "15 727335.3770936322 -3053.2790866956348 19 1.1494742132376222e-07\n",
      "16 724262.999090775 -3072.3780028572073 19 1.1609689553699984e-07\n",
      "17 721171.7668415105 -3091.232249264489 18 1.1725786449236985e-07\n",
      "18 718062.5421173429 -3109.2247241676087 18 1.1843044313729355e-07\n",
      "19 714935.4975958773 -3127.0445214656647 15 1.1961474756866649e-07\n",
      "20 711792.574588047 -3142.923007830279 15 1.2081089504435316e-07\n",
      "21 708630.7989494732 -3161.775638573803 15 1.220190039947967e-07\n",
      "22 705450.2785793255 -3180.5203701476566 14 1.2323919403474465e-07\n",
      "23 702252.2256727137 -3198.052906611818 14 1.244715859750921e-07\n",
      "24 699035.5309674175 -3216.6947052961914 14 1.2571630183484302e-07\n",
      "25 695800.276153932 -3235.254813485546 14 1.2697346485319146e-07\n",
      "26 692547.113049054 -3253.1631048779236 13 1.2824319950172337e-07\n",
      "27 689276.211229149 -3270.9018199050333 13 1.295256314967406e-07\n",
      "28 685987.3627554046 -3288.848473744467 11 1.30820887811708e-07\n",
      "29 682682.1882575973 -3305.174497807282 11 1.321290966898251e-07\n",
      "30 679359.1584999948 -3323.0297576024896 10 1.3345038765672335e-07\n",
      "31 676019.0153959088 -3340.1431040859316 10 1.347848915332906e-07\n",
      "32 672661.0620837249 -3357.953312183963 10 1.361327404486235e-07\n",
      "33 669285.4338449489 -3375.628238776 10 1.3749406785310975e-07\n",
      "34 665892.2736129002 -3393.1602320487145 10 1.3886900853164084e-07\n",
      "35 662482.2727685996 -3410.000844300608 9 1.4025769861695725e-07\n",
      "36 659055.7326217531 -3426.5401468464406 9 1.4166027560312682e-07\n",
      "37 655612.202982692 -3443.5296390610747 8 1.430768783591581e-07\n",
      "38 652152.6632516715 -3459.5397310205735 8 1.4450764714274967e-07\n",
      "39 648676.4209532903 -3476.2422983811703 8 1.4595272361417717e-07\n",
      "40 645183.6687077604 -3492.752245529904 8 1.4741225085031894e-07\n",
      "41 641674.6082903703 -3509.0604173900792 8 1.4888637335882214e-07\n",
      "42 638149.4508683507 -3525.1574220196344 8 1.5037523709241035e-07\n",
      "43 634608.417243005 -3541.033625345677 8 1.5187898946333445e-07\n",
      "44 631052.3046761826 -3556.1125668224413 7 1.533977793579678e-07\n",
      "45 627481.43113799 -3570.8735381925944 7 1.5493175715154747e-07\n",
      "46 623895.3967129254 -3586.034425064572 7 1.5648107472306294e-07\n",
      "47 620294.6197340782 -3600.7769788472215 6 1.5804588547029357e-07\n",
      "48 616680.0886075676 -3614.5311265105847 6 1.5962634432499652e-07\n",
      "49 613051.209202682 -3628.879404885578 6 1.6122260776824649e-07\n",
      "50 609408.2761569947 -3642.9330456872704 6 1.6283483384592896e-07\n",
      "51 605751.595860677 -3656.6802963177906 6 1.6446318218438825e-07\n",
      "52 602081.4867510276 -3670.1091096493183 6 1.6610781400623213e-07\n",
      "53 598398.279613506 -3683.207137521589 6 1.6776889214629445e-07\n",
      "54 594702.5666181771 -3695.7129953289405 5 1.694465810677574e-07\n",
      "55 590995.4003660318 -3707.166252145311 4 1.71141046878435e-07\n",
      "56 587276.7950142813 -3718.605351750506 4 1.7285245734721935e-07\n",
      "57 583546.5318249891 -3730.2631892921636 4 1.7458098192069155e-07\n",
      "58 579805.0073369211 -3741.5244880680693 4 1.7632679173989847e-07\n",
      "59 576052.6323678836 -3752.3749690374825 4 1.7809005965729747e-07\n",
      "60 572289.8323648219 -3762.800003061653 4 1.7987096025387044e-07\n",
      "61 568517.04776158 -3772.7846032419475 4 1.8166966985640915e-07\n",
      "62 564734.7343444948 -3782.3134170852136 4 1.8348636655497324e-07\n",
      "63 560943.3636259738 -3791.3707185209496 4 1.8532123022052297e-07\n",
      "64 557143.4232262403 -3799.9403997334884 4 1.871744425227282e-07\n",
      "65 553335.4172634063 -3808.005962833995 4 1.8904618694795546e-07\n",
      "66 549519.8667520387 -3815.550511367619 4 1.9093664881743503e-07\n",
      "67 545697.3100104311 -3822.5567416076083 4 1.928460153056094e-07\n",
      "68 541868.4097981851 -3828.9002122459933 3 1.9477447545866548e-07\n",
      "69 538034.296517822 -3834.113280363148 3 1.9672222021325215e-07\n",
      "70 534194.8920173453 -3839.4045004766667 3 1.9868944241538467e-07\n",
      "71 530350.8076877976 -3844.08432954771 3 2.0067633683953852e-07\n",
      "72 526502.6743866291 -3848.1333011684474 3 2.0268310020793392e-07\n",
      "73 522651.14290074504 -3851.531485884101 3 2.0470993121001327e-07\n",
      "74 518796.94821777666 -3854.194682968373 2 2.067570305221134e-07\n",
      "75 514940.81087208935 -3856.1373456873116 2 2.0882460082733453e-07\n",
      "76 511083.4136772606 -3857.3971948287217 1 2.1091284683560788e-07\n",
      "77 507225.49964634643 -3857.914030914195 1 2.1302197530396397e-07\n",
      "78 503367.7609821176 -3857.7386642288184 1 2.151521950570036e-07\n",
      "79 499510.97840383137 -3856.782578286249 1 2.1730371700757364e-07\n",
      "80 495655.95609514666 -3855.022308684711 1 2.194767541776494e-07\n",
      "81 491803.9035485493 -3852.052546597377 0 2.2167152171942588e-07\n",
      "82 487955.686032562 -3848.217515987286 0 2.2388823693662014e-07\n",
      "83 484111.770652753 -3843.915379808983 0 2.2612711930598634e-07\n",
      "84 480273.0603951384 -3838.7102576146135 0 2.2838839049904622e-07\n",
      "85 476440.4845608912 -3832.5758342472254 0 2.3067227440403668e-07\n",
      "86 472614.9993769162 -3825.485183974961 0 2.3297899714807706e-07\n",
      "87 468797.5886195272 -3817.410757389036 0 2.3530878711955783e-07\n",
      "88 464989.26425148704 -3808.3243680401356 0 2.3766187499075342e-07\n",
      "89 461191.067072678 -3798.1971788090304 0 2.4003849374066097e-07\n",
      "90 457404.0673846947 -3786.999687983305 0 2.424388786780676e-07\n",
      "91 453629.3656696419 -3774.7017150528263 0 2.4486326746484827e-07\n",
      "92 449868.09328344354 -3761.2723861983395 0 2.4731190013949676e-07\n",
      "93 446121.41316394205 -3746.680119501485 0 2.4978501914089175e-07\n",
      "94 442390.52055411675 -3730.8926098252996 0 2.5228286933230067e-07\n",
      "95 438676.6437407108 -3713.87681340595 0 2.548056980256237e-07\n",
      "96 434981.04480861116 -3695.598932099645 0 2.573537550058799e-07\n",
      "97 431305.0298011494 -3676.0150074617704 1 2.5992729255593874e-07\n",
      "98 427649.92591916875 -3655.1038819806417 0 2.6252656548149814e-07\n",
      "99 424017.07618564944 -3632.8497335193097 0 2.651518311363131e-07\n",
      "100 420407.9062452471 -3609.169940402324 0 2.6780334944767626e-07\n",
      "101 416823.86390801147 -3584.0423372356454 1 2.70481382942153e-07\n",
      "102 413266.4135801068 -3557.450327904662 0 2.7318619677157453e-07\n",
      "103 409737.08923737356 -3529.324342733249 0 2.759180587392903e-07\n",
      "104 406237.45203092194 -3499.637206451618 0 2.786772393266832e-07\n",
      "105 402769.1049377432 -3468.347093178716 0 2.8146401171995e-07\n",
      "106 399333.69321732933 -3435.4117204138893 0 2.8427865183714953e-07\n",
      "107 395932.9053621625 -3400.7878551668255 0 2.8712143835552103e-07\n",
      "108 392568.47406834643 -3364.431293816073 0 2.899926527390762e-07\n",
      "109 389242.1772267335 -3326.296841612959 0 2.92892579266467e-07\n",
      "110 385955.8389350265 -3286.338291706983 0 2.9582150505913165e-07\n",
      "111 382711.3305312399 -3244.5084037865745 0 2.98779720109723e-07\n",
      "112 379510.5716490027 -3200.758882237191 0 3.017675173108202e-07\n",
      "113 376355.53129509726 -3155.040353905468 0 3.047851924839284e-07\n",
      "114 373248.2289497645 -3107.302345332748 0 3.0783304440876767e-07\n",
      "115 370190.7356901733 -3057.493259591225 0 3.109113748528554e-07\n",
      "116 367185.17533757375 -3005.5603525995393 0 3.1402048860138394e-07\n",
      "117 364233.7256286141 -2951.44970895967 0 3.171606934873978e-07\n",
      "118 361338.61941129813 -2895.1062173159444 0 3.2033230042227175e-07\n",
      "119 358502.1458661149 -2836.473545183253 0 3.235356234264945e-07\n",
      "120 355726.6517528463 -2775.4941132685635 0 3.2677097966075946e-07\n",
      "121 353014.5426835745 -2712.109069271828 0 3.3003868945736705e-07\n",
      "122 350368.2844224408 -2646.258261133684 0 3.3333907635194074e-07\n",
      "123 347790.404212694 -2577.8802097468288 0 3.3667246711546013e-07\n",
      "124 345283.49213159044 -2506.912081103539 0 3.400391917866147e-07\n",
      "125 342850.20247372566 -2433.289657864778 0 3.434395837044809e-07\n",
      "126 340493.2551633665 -2356.947310359159 0 3.468739795415257e-07\n",
      "127 338215.4371963923 -2277.817966974224 0 3.50342719336941e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 336019.60411245143 -2195.8330839408445 0 3.538461465303104e-07\n",
      "129 333908.6814979336 -2110.9226145178545 0 3.573846079956135e-07\n",
      "130 331885.66652041767 -2023.014977515908 0 3.6095845407556966e-07\n",
      "131 329953.6294952233 -1932.0370251943823 0 3.645680386163254e-07\n",
      "132 328115.71548472636 -1837.9140104969265 0 3.6821371900248867e-07\n",
      "133 326375.14593111014 -1740.5695536162239 0 3.7189585619251353e-07\n",
      "134 324735.22032324364 -1639.9256078664912 0 3.7561481475443865e-07\n",
      "135 323199.3178983741 -1535.902424869535 0 3.7937096290198303e-07\n",
      "136 321770.89937935583 -1428.4185190182761 0 3.8316467253100284e-07\n",
      "137 320453.5087481455 -1317.39063121035 0 3.8699631925631286e-07\n",
      "138 319250.77505628497 -1202.7336918605142 0 3.9086628244887597e-07\n",
      "139 318166.4142731662 -1084.3607831187546 0 3.947749452733647e-07\n",
      "140 317204.2311728165 -962.1831003497355 0 3.987226947260984e-07\n",
      "141 316368.12126000365 -836.1099128128262 0 4.027099216733594e-07\n",
      "142 315662.07273647375 -706.0485235298984 0 4.0673702089009297e-07\n",
      "143 315090.1685081384 -571.9042283353629 0 4.108043910989939e-07\n",
      "144 314656.5882340366 -433.5802741018124 0 4.1491243500998386e-07\n",
      "145 314365.6104179401 -290.977816096507 0 4.190615593600837e-07\n",
      "146 314221.6145434701 -143.9958744699834 0 4.2325217495368454e-07\n",
      "147 314206.14074073086 -15.47380273923045 0 2.1374234835161068e-07\n",
      "148 314206.1407407308 -5.820766091346741e-11 0 3.141464139263943e-18\n",
      "LMNN converged with objective 314206.1407407308\n",
      "Mean average precision: 0.4047719800041228\n",
      "Top1: 0.4421428571428571\n",
      "Top5: 0.645\n",
      "Top10: 0.7321428571428571\n",
      "Top5-Precision: 0.31857142857142856\n",
      "Top5-Recall: 0.4198809523809523\n"
     ]
    }
   ],
   "source": [
    "LargeMarg_Model = LMNN().fit(train_feat,train_labels)\n",
    "query_feat_LMNN = LargeMarg_Model.transform(query_feat)\n",
    "gallery_feat_LMNN = LargeMarg_Model.transform(gallery_feat)\n",
    "\n",
    "kNN(query_feat_LMNN, gallery_feat_LMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Metric Learning for Kernel Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on all dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLKR]\n",
      "[MLKR]  Iteration      Objective Value    Time(s)\n",
      "[MLKR] ------------------------------------------\n",
      "[MLKR]          0         2.116000e+03      73.81\n",
      "[MLKR] Training took    77.14s.\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "MLKR_Model = MLKR(verbose=True).fit(train_noval_feat,train_noval_labels)\n",
    "val_query_feat_MLKR = MLKR_Model.transform(val_query_feat)\n",
    "val_gallery_feat_MLKR = MLKR_Model.transform(val_gallery_feat)\n",
    "kNN_val(val_query_feat_MLKR,val_gallery_feat_MLKR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation on reduced dimensions - PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLKR]\n",
      "[MLKR]  Iteration      Objective Value    Time(s)\n",
      "[MLKR] ------------------------------------------\n",
      "[MLKR]          0         2.042865e+03      11.82\n",
      "[MLKR]          1         1.836993e+06      12.60\n",
      "[MLKR]          2         1.121481e+06      13.12\n",
      "[MLKR]          3         1.960010e+03      11.90\n",
      "[MLKR]          4         1.408250e+03      11.91\n",
      "[MLKR]          5         1.121481e+06      12.84\n",
      "[MLKR]          6         2.869980e+01      13.30\n",
      "[MLKR]          7         3.805442e+01      12.51\n",
      "[MLKR]          8         2.869980e+01      11.87\n",
      "[MLKR]          9         1.741339e+09      13.61\n",
      "[MLKR]         10         1.741339e+09      12.14\n",
      "[MLKR]         11         1.741339e+09      11.62\n",
      "[MLKR]         12         1.740813e+09      13.15\n",
      "[MLKR]         13         1.724821e+09      11.06\n",
      "[MLKR]         14         5.757777e+08      13.82\n",
      "[MLKR]         15         1.121481e+06      12.00\n",
      "[MLKR]         16         1.098394e+06      11.78\n",
      "[MLKR]         17         2.697753e+01      11.75\n",
      "[MLKR]         18         5.417649e-03      11.76\n",
      "[MLKR]         19         2.034868e-04      11.93\n",
      "[MLKR]         20         1.267791e-07      11.62\n",
      "[MLKR]         21         1.257105e-07      11.88\n",
      "[MLKR] Training took   272.36s.\n",
      "PCA-MLKR with 50 dimensions:\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n",
      "[MLKR]\n",
      "[MLKR]  Iteration      Objective Value    Time(s)\n",
      "[MLKR] ------------------------------------------\n",
      "[MLKR]          0         2.116000e+03      24.47\n",
      "[MLKR] Training took    24.76s.\n",
      "PCA-MLKR with 500 dimensions:\n",
      "Mean average precision: 1.0\n",
      "Top1: 1.0\n",
      "Top5: 1.0\n",
      "Top10: 1.0\n",
      "Top5-Precision: 0.7547738693467336\n",
      "Top5-Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "MLKR_Model = MLKR(verbose=True).fit(train_noval_feat_PCA_50,train_noval_labels)\n",
    "val_query_feat_MLKR = MLKR_Model.transform(val_query_feat_PCA_50)\n",
    "val_gallery_feat_MLKR = MLKR_Model.transform(val_gallery_feat_PCA_50)\n",
    "print(\"PCA-MLKR with 50 dimensions:\")\n",
    "kNN_val(val_query_feat_MLKR,val_gallery_feat_MLKR)\n",
    "\n",
    "MLKR_Model = MLKR(verbose=True).fit(train_noval_feat_PCA_500,train_noval_labels)\n",
    "val_query_feat_MLKR = MLKR_Model.transform(val_query_feat_PCA_500)\n",
    "val_gallery_feat_MLKR = MLKR_Model.transform(val_gallery_feat_PCA_500)\n",
    "print(\"PCA-MLKR with 500 dimensions:\")\n",
    "kNN_val(val_query_feat_MLKR,val_gallery_feat_MLKR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with reduced features - PCA 500 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLKR]\n",
      "[MLKR]  Iteration      Objective Value    Time(s)\n",
      "[MLKR] ------------------------------------------\n",
      "[MLKR]          0         2.116000e+03      31.23\n",
      "[MLKR] Training took    31.52s.\n",
      "PCA-MLKR with 500 dimensions\n",
      "Mean average precision: 0.4354747990105133\n",
      "Top1: 0.46785714285714286\n",
      "Top5: 0.6707142857142857\n",
      "Top10: 0.7492857142857143\n",
      "Top5-Precision: 0.34271428571428575\n",
      "Top5-Recall: 0.45077380952380947\n"
     ]
    }
   ],
   "source": [
    "MLKR_Model = MLKR(verbose=True).fit(train_feat_PCA_500,train_labels)\n",
    "query_feat_MLKR = MLKR_Model.transform(query_feat_PCA_500)\n",
    "gallery_feat_MLKR = MLKR_Model.transform(gallery_feat_PCA_500)\n",
    "print(\"PCA-MLKR with 500 dimensions\")\n",
    "kNN(query_feat_MLKR,gallery_feat_MLKR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLKR]\n",
      "[MLKR]  Iteration      Objective Value    Time(s)\n",
      "[MLKR] ------------------------------------------\n",
      "[MLKR]          0         2.116000e+03      87.70\n",
      "[MLKR] Training took    90.69s.\n",
      "Mean average precision: 0.4353663677592249\n",
      "Top1: 0.47\n",
      "Top5: 0.6685714285714286\n",
      "Top10: 0.7492857142857143\n",
      "Top5-Precision: 0.3425714285714286\n",
      "Top5-Recall: 0.4498214285714286\n"
     ]
    }
   ],
   "source": [
    "MLKR_Model = MLKR(verbose=True).fit(train_feat,train_labels)\n",
    "query_feat_MLKR = MLKR_Model.transform(query_feat)\n",
    "gallery_feat_MLKR = MLKR_Model.transform(gallery_feat)\n",
    "\n",
    "kNN(query_feat_MLKR, gallery_feat_MLKR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected Neural Network with Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an auxiliary matrix for training\n",
    "train_matrix = np.append(train_feat, np.reshape(train_labels, (7368,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Define necessary triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Neural Network with activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69ab8b1b3dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_person_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbool_idx_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0munique_id_NN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool_idx_NN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool_idx_NN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_image_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_input = 2048   # input layer (28x28 pixels)\n",
    "n_hidden1 = 1024 # 1st hidden layer\n",
    "n_hidden2 = 512 # 2nd hidden layer\n",
    "n_hidden3 = 256 #3rd hidden layer\n",
    "n_output = 256   # output layer (0-9 digits)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_iterations = 10000\n",
    "batch_person_size = 72\n",
    "batch_image_size = 4\n",
    "batch_size = batch_person_size * batch_image_size\n",
    "dropout = 0.5\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None])\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "layer_1 = tf.layers.dense(X, n_hidden1, tf.nn.relu)\n",
    "layer_norm_1 = tf.layers.batch_normalization(layer_1)\n",
    "layer_drop_1 = tf.nn.dropout(layer_norm_1, keep_prob)\n",
    "layer_2 = tf.layers.dense(layer_drop_1, n_hidden2)\n",
    "layer_norm_2 = tf.layers.batch_normalization(layer_2) #layer not used for final selection\n",
    "layer_3 = tf.layers.dense(layer_norm_2, n_hidden3) #layer not used for final selection\n",
    "layer_drop_3 = tf.nn.dropout(layer_3, keep_prob) #layer not used for final selection\n",
    "output_layer = tf.layers.dense(layer_2, n_output)\n",
    "\n",
    "hard_triplet_loss = batch_hard_triplet_loss(Y, output_layer, 100)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(hard_triplet_loss)\n",
    "\n",
    "features = output_layer\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "unique_id_train = np.unique(train_labels) #find all unique training IDs\n",
    "\n",
    "# train on mini batches\n",
    "for i in range(n_iterations):\n",
    "    batch_x = np.zeros((batch_size,2048))\n",
    "    batch_y = np.zeros(batch_size)\n",
    "    np.random.shuffle(train_matrix)\n",
    "    unique_id_NN = unique_id_train[np.array(random.sample(range(unique_id_train.shape[0]),batch_person_size))] # pick 18 random unique training IDs, random.sample does not work on numpy array\n",
    "    for j in range(batch_person_size):\n",
    "        bool_idx_NN = train_matrix[:,2048] == unique_id_NN[j]\n",
    "        batch_x[j*batch_image_size:(j+1)*batch_image_size,:] = train_matrix[bool_idx_NN,:2048][:batch_image_size,:]\n",
    "        batch_y[j*batch_image_size:(j+1)*batch_image_size] = train_matrix[bool_idx_NN,2048][:batch_image_size]\n",
    "        \n",
    "        \n",
    "    \n",
    "    batch_x, batch_y = train_matrix[:batch_size,:2048],train_matrix[:batch_size,2048]\n",
    "    sess.run(train_step, feed_dict={X: batch_x, Y: batch_y, keep_prob:dropout})\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(\"Iteration\", str(i))\n",
    "        \n",
    "gallery_feat_Neural = sess.run(features, feed_dict={X: gallery_feat, Y: gallery_labels, keep_prob:1.0})\n",
    "query_feat_Neural = sess.run(features, feed_dict={X: query_feat, Y: query_labels, keep_prob:1.0})\n",
    "\n",
    "kNN(query_feat_Neural, gallery_feat_Neural)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Linear Neural Network without activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 2048   # input layer (28x28 pixels)\n",
    "n_hidden1 = 1024 # 1st hidden layer\n",
    "n_hidden2 = 512 # 2nd hidden layer\n",
    "n_hidden3 = 512 #3rd hidden layer\n",
    "n_output = 500   # output layer (0-9 digits)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_iterations = 5000\n",
    "batch_person_size = 72\n",
    "batch_image_size = 4\n",
    "batch_size = batch_person_size * batch_image_size\n",
    "dropout = 0.5\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None])\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "layer_1 = tf.layers.dense(X, n_hidden1)\n",
    "layer_2 = tf.layers.dense(layer_1, n_hidden2)\n",
    "layer_3 = tf.layers.dense(layer_2, n_hidden3) \n",
    "output_layer = tf.layers.dense(layer_3, n_output)\n",
    "\n",
    "hard_triplet_loss = batch_hard_triplet_loss(Y, output_layer, 200)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(hard_triplet_loss)\n",
    "\n",
    "features = output_layer\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "unique_id_train = np.unique(train_labels) #find all unique training IDs\n",
    "\n",
    "# train on mini batches\n",
    "for i in range(n_iterations):\n",
    "    batch_x = np.zeros((batch_size,2048))\n",
    "    batch_y = np.zeros(batch_size)\n",
    "    np.random.shuffle(train_matrix)\n",
    "    unique_id_NN = unique_id_train[np.array(random.sample(range(unique_id_train.shape[0]),batch_person_size))] # pick 18 random unique training IDs, random.sample does not work on numpy array\n",
    "    for j in range(batch_person_size):\n",
    "        bool_idx_NN = train_matrix[:,2048] == unique_id_NN[j]\n",
    "        batch_x[j*batch_image_size:(j+1)*batch_image_size,:] = train_matrix[bool_idx_NN,:2048][:batch_image_size,:]\n",
    "        batch_y[j*batch_image_size:(j+1)*batch_image_size] = train_matrix[bool_idx_NN,2048][:batch_image_size]\n",
    "        \n",
    "        \n",
    "    \n",
    "    batch_x, batch_y = train_matrix[:batch_size,:2048],train_matrix[:batch_size,2048]\n",
    "    sess.run(train_step, feed_dict={X: batch_x, Y: batch_y, keep_prob:dropout})\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(\"Iteration\", str(i))\n",
    "        \n",
    "gallery_feat_Neural = sess.run(features, feed_dict={X: gallery_feat, Y: gallery_labels, keep_prob:1.0})\n",
    "query_feat_Neural = sess.run(features, feed_dict={X: query_feat, Y: query_labels, keep_prob:1.0})\n",
    "\n",
    "kNN(query_feat_Neural, gallery_feat_Neural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPENDIX A: Demonstration of mAP calculation (with graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be deleted, it is not necessary to duplicate it\n",
    "\n",
    "# k nearest neighbours implementation\n",
    "\n",
    "k = 10\n",
    "top_k_rank = np.zeros((query_labels.shape[0],k))\n",
    "top_k_precision = np.zeros((query_labels.shape[0],k))\n",
    "top_k_recall = np.zeros((query_labels.shape[0],k))\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=20).fit(gallery_feat)\n",
    "distances, indices = nbrs.kneighbors(query_feat)\n",
    "\n",
    "# Indices contains indices of the closes gallery pictures to each query image\n",
    "\n",
    "# Select the images where camId and label not the same as of the query image\n",
    "# Take top k of them\n",
    "\n",
    "for i in range (0, query_labels.shape[0], 1):\n",
    "    selected_indices = np.logical_not(np.logical_and(gallery_camId[indices[i, :]] == query_camId[i], gallery_labels[indices[i,:]] == query_labels[i]))\n",
    "    number_of_removed = np.sum(np.logical_and(gallery_camId == query_camId[i], gallery_labels == query_labels[i]))\n",
    "    removed_indices = indices [i, selected_indices]\n",
    "    is_same_label = (query_labels[i] == gallery_labels[removed_indices])\n",
    "    \n",
    "    for j in range (0, k, 1):\n",
    "        top_k_rank[i,j] = np.sum(is_same_label[:(j+1)]) != 0\n",
    "        top_k_precision[i,j]=np.sum(is_same_label[:(j+1)])/(j+1)\n",
    "        top_k_recall[i,j] = np.sum(is_same_label[:(j+1)])/(np.sum(gallery_labels==query_labels[i])-number_of_removed)\n",
    "\n",
    "print(\"Top1:\", top_k_rank[:, 0].mean())\n",
    "print(\"Top5:\", top_k_rank[:, 4].mean())\n",
    "print(\"Top10:\", top_k_rank[:, 9].mean())\n",
    "print(\"Top5-Precision:\", top_k_precision[:, 4].mean())\n",
    "print(\"Top5-Recall:\", top_k_recall[:, 4].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average precision calculation demonstration\n",
    "# Try index 1100, that is representative\n",
    "\n",
    "recall = top_k_recall[1100, :]\n",
    "precision = top_k_precision[1100, :]\n",
    "print (recall)\n",
    "print (precision)\n",
    "\n",
    "# Just plot the recall, precision values\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, 'rD')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# Flat out the variation (not used in any calculations!)\n",
    "index = (recall).argsort()[::-1] # argsort returns the indices that would sort an array (in this case the vector recall)\n",
    "recall = recall[index]\n",
    "precision = precision[index]\n",
    "for i in range (0, recall.shape[0], 1):\n",
    "    precision[i] = np.max(precision[:(i+1)])\n",
    "print (recall)\n",
    "print (precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, 'rD')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean average precision values\n",
    "recall = top_k_recall[1100, :]\n",
    "precision = top_k_precision[1100, :]\n",
    "\n",
    "index = (recall).argsort()[::-1] # argsort returns the indices that would sort an array (in this case the vector recall)\n",
    "recall = recall[index]\n",
    "precision = precision[index]\n",
    "recall_range = np.arange(0, 1.1, 0.1)\n",
    "precision_range = np.zeros((recall_range.shape))\n",
    "for i in range (0, recall_range.shape[0], 1):\n",
    "    if (precision[recall>=recall_range[i]].size != 0):\n",
    "        precision_range[i] = np.max(precision[recall>=recall_range[i]])\n",
    "    else:     \n",
    "        precision_range[i] = 0\n",
    "print (precision_range)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall_range, precision_range, 'rD')\n",
    "plt.xlabel('Range of recall values')\n",
    "plt.ylabel('Maximum precision')\n",
    "plt.show()\n",
    "\n",
    "ap = np.mean(precision_range)\n",
    "print (ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "average_precisions = np.zeros((query_labels.shape))\n",
    "for j in range (0, query_labels.shape[0], 1):\n",
    "    recall = top_k_accuracies_recall[j, :]\n",
    "    precision = top_k_accuracies_classical[j, :]\n",
    "    index = (recall).argsort()[::-1] # argsort returns the indices that would sort an array (in this case the vector recall)\n",
    "    recall = recall[index]\n",
    "    precision = precision[index]\n",
    "    recall_range = np.arange(0, 1.1, 0.1)\n",
    "    precision_range = np.zeros((recall_range.shape))\n",
    "    for i in range (0, recall_range.shape[0], 1):    \n",
    "        if (precision[recall>=recall_range[i]].size != 0):\n",
    "            precision_range[i] = np.max(precision[recall>=recall_range[i]])\n",
    "        else:     \n",
    "            precision_range[i] = 0\n",
    "    average_precisions[j] = np.mean(precision_range)\n",
    "print (average_precisions, average_precisions.shape)\n",
    "print (average_precisions.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
